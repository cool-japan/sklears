//! Quadratic Discriminant Analysis (QDA) implementation

// ✅ Using SciRS2 dependencies following SciRS2 policy
use scirs2_core::ndarray::{Array1, Array2, ArrayView1, Axis};
use sklears_core::{
    error::{validate, Result},
    prelude::SklearsError,
    traits::{Estimator, Fit, Predict, PredictProba, Trained, Untrained},
    types::Float,
};
use std::marker::PhantomData;

/// Configuration for Quadratic Discriminant Analysis
#[derive(Debug, Clone)]
pub struct QuadraticDiscriminantAnalysisConfig {
    /// Prior probabilities of the classes
    pub priors: Option<Array1<Float>>,
    /// Regularization parameter for covariance matrices
    pub reg_param: Float,
    /// Whether to store the covariance matrices
    pub store_covariance: bool,
    /// Tolerance for stopping criteria
    pub tol: Float,
    /// Whether to use diagonal covariance matrices (for high-dimensional data)
    pub diagonal_covariance: bool,
    /// Whether to use robust estimation
    pub robust: bool,
    /// Robust estimation method
    pub robust_method: String,
    /// Contamination fraction for robust estimation
    pub contamination: Float,
}

impl Default for QuadraticDiscriminantAnalysisConfig {
    fn default() -> Self {
        Self {
            priors: None,
            reg_param: 0.0,
            store_covariance: false,
            tol: 1e-4,
            diagonal_covariance: false,
            robust: false,
            robust_method: "mcd".to_string(),
            contamination: 0.1,
        }
    }
}

/// Quadratic Discriminant Analysis
///
/// A classifier with a quadratic decision boundary, generated by fitting class
/// conditional densities to the data and using Bayes' rule. Each class has its
/// own covariance matrix.
#[derive(Debug, Clone)]
pub struct QuadraticDiscriminantAnalysis<State = Untrained> {
    config: QuadraticDiscriminantAnalysisConfig,
    state: PhantomData<State>,
    // Trained state fields
    classes_: Option<Array1<i32>>,
    means_: Option<Array2<Float>>,
    covariances_: Option<Vec<Array2<Float>>>,
    priors_: Option<Array1<Float>>,
    n_features_: Option<usize>,
}

impl QuadraticDiscriminantAnalysis<Untrained> {
    /// Create a new QuadraticDiscriminantAnalysis instance
    pub fn new() -> Self {
        Self {
            config: QuadraticDiscriminantAnalysisConfig::default(),
            state: PhantomData,
            classes_: None,
            means_: None,
            covariances_: None,
            priors_: None,
            n_features_: None,
        }
    }

    /// Set the prior probabilities
    pub fn priors(mut self, priors: Option<Array1<Float>>) -> Self {
        self.config.priors = priors;
        self
    }

    /// Set the regularization parameter
    pub fn reg_param(mut self, reg_param: Float) -> Self {
        self.config.reg_param = reg_param;
        self
    }

    /// Set whether to store covariance matrices
    pub fn store_covariance(mut self, store_covariance: bool) -> Self {
        self.config.store_covariance = store_covariance;
        self
    }

    /// Set the tolerance
    pub fn tol(mut self, tol: Float) -> Self {
        self.config.tol = tol;
        self
    }

    /// Set whether to use diagonal covariance matrices
    pub fn diagonal_covariance(mut self, diagonal_covariance: bool) -> Self {
        self.config.diagonal_covariance = diagonal_covariance;
        self
    }

    /// Set whether to use robust estimation
    pub fn robust(mut self, robust: bool) -> Self {
        self.config.robust = robust;
        self
    }

    /// Set the robust estimation method
    pub fn robust_method(mut self, robust_method: &str) -> Self {
        self.config.robust_method = robust_method.to_string();
        self
    }

    /// Set the contamination fraction for robust estimation
    pub fn contamination(mut self, contamination: Float) -> Self {
        self.config.contamination = contamination;
        self
    }

    /// Compute robust mean using iterative reweighting
    fn robust_mean(&self, data: &Array2<Float>, weights: &Array1<Float>) -> Array1<Float> {
        let (_n_samples, n_features) = data.dim();
        let mut robust_mean = Array1::zeros(n_features);

        for j in 0..n_features {
            let weighted_sum: Float = data
                .column(j)
                .iter()
                .zip(weights.iter())
                .map(|(&x, &w)| x * w)
                .sum();
            let weight_sum: Float = weights.sum();

            if weight_sum > 0.0 {
                robust_mean[j] = weighted_sum / weight_sum;
            }
        }

        robust_mean
    }

    /// Compute robust covariance using Minimum Covariance Determinant (MCD)
    fn robust_covariance_mcd(
        &self,
        data: &Array2<Float>,
    ) -> Result<(Array1<Float>, Array2<Float>, Array1<Float>)> {
        let (n_samples, n_features) = data.dim();
        let subset_size = ((n_samples as Float * (1.0 - self.config.contamination)).ceil()
            as usize)
            .max(n_features + 1)
            .min(n_samples);

        let mut best_mean = data.mean_axis(Axis(0)).unwrap();
        let mut best_cov = Array2::eye(n_features);
        let mut best_det = Float::INFINITY;
        let mut best_weights = Array1::ones(n_samples);

        // Try multiple random subsets
        for trial in 0..50 {
            // Generate random subset indices
            let mut indices: Vec<usize> = (0..n_samples).collect();

            // Simple pseudo-random shuffle using trial number
            for i in 0..n_samples {
                let j = (i + trial * 17) % n_samples;
                indices.swap(i, j);
            }
            indices.truncate(subset_size);

            // Compute subset mean and covariance
            let subset_data: Array2<Float> =
                Array2::from_shape_fn((subset_size, n_features), |(i, j)| data[[indices[i], j]]);

            let subset_mean = subset_data.mean_axis(Axis(0)).unwrap();
            let mut subset_cov = Array2::zeros((n_features, n_features));

            // Compute covariance for subset
            for i in 0..subset_size {
                let row = subset_data.row(i);
                let diff = &row - &subset_mean;
                for j in 0..n_features {
                    for k in 0..n_features {
                        subset_cov[[j, k]] += diff[j] * diff[k];
                    }
                }
            }

            if subset_size > 1 {
                subset_cov /= (subset_size - 1) as Float;
            }

            // Add regularization for numerical stability
            for i in 0..n_features {
                subset_cov[[i, i]] += self.config.tol;
            }

            // Compute determinant (simplified - use product of diagonal)
            let det: Float = subset_cov.diag().iter().product();

            if det > 0.0 && det < best_det {
                best_det = det;
                best_mean = subset_mean;
                best_cov = subset_cov;

                // Compute Mahalanobis distances and weights
                let cov_inv = self.pseudo_inverse(&best_cov)?;
                for i in 0..n_samples {
                    let sample = data.row(i);
                    let diff = &sample - &best_mean;
                    let mahal_dist = diff.dot(&cov_inv.dot(&diff)).sqrt();

                    // Robust weight using Tukey's biweight function
                    let c = 4.685; // Tuning constant
                    if mahal_dist <= c {
                        let u = mahal_dist / c;
                        best_weights[i] = (1.0 - u * u).powi(2);
                    } else {
                        best_weights[i] = 0.0;
                    }
                }
            }
        }

        Ok((best_mean, best_cov, best_weights))
    }

    /// Compute pseudo-inverse using LU decomposition with partial pivoting
    fn pseudo_inverse(&self, matrix: &Array2<Float>) -> Result<Array2<Float>> {
        let n = matrix.nrows();
        let mut a = matrix.clone();
        let mut b = Array2::eye(n);

        // Forward elimination with partial pivoting
        for i in 0..n {
            // Find pivot
            let mut max_row = i;
            for k in (i + 1)..n {
                if a[[k, i]].abs() > a[[max_row, i]].abs() {
                    max_row = k;
                }
            }

            // Swap rows if needed
            if max_row != i {
                for j in 0..n {
                    a.swap([i, j], [max_row, j]);
                    b.swap([i, j], [max_row, j]);
                }
            }

            // Check for singularity
            if a[[i, i]].abs() < self.config.tol {
                // Add regularization to diagonal
                a[[i, i]] += self.config.reg_param + self.config.tol;
            }

            // Eliminate column
            for k in (i + 1)..n {
                let factor = a[[k, i]] / a[[i, i]];
                for j in i..n {
                    a[[k, j]] -= factor * a[[i, j]];
                }
                for j in 0..n {
                    b[[k, j]] -= factor * b[[i, j]];
                }
            }
        }

        // Back substitution
        for i in (0..n).rev() {
            for j in 0..n {
                for k in (i + 1)..n {
                    b[[i, j]] -= a[[i, k]] * b[[k, j]];
                }
                b[[i, j]] /= a[[i, i]];
            }
        }

        Ok(b)
    }
}

impl Default for QuadraticDiscriminantAnalysis<Untrained> {
    fn default() -> Self {
        Self::new()
    }
}

impl Estimator for QuadraticDiscriminantAnalysis<Untrained> {
    type Config = QuadraticDiscriminantAnalysisConfig;
    type Error = SklearsError;
    type Float = Float;

    fn config(&self) -> &Self::Config {
        &self.config
    }
}

impl Fit<Array2<Float>, Array1<i32>> for QuadraticDiscriminantAnalysis<Untrained> {
    type Fitted = QuadraticDiscriminantAnalysis<Trained>;

    fn fit(self, x: &Array2<Float>, y: &Array1<i32>) -> Result<Self::Fitted> {
        // Basic validation
        validate::check_consistent_length(x, y)?;

        let (n_samples, n_features) = x.dim();
        if n_samples == 0 {
            return Err(SklearsError::InvalidInput(
                "No samples provided".to_string(),
            ));
        }

        // Get unique classes
        let mut classes: Vec<i32> = y
            .iter()
            .cloned()
            .collect::<std::collections::HashSet<_>>()
            .into_iter()
            .collect();
        classes.sort();
        let n_classes = classes.len();

        if n_classes < 2 {
            return Err(SklearsError::InvalidInput(
                "Need at least 2 classes".to_string(),
            ));
        }

        // Calculate class means and covariances
        let mut means = Array2::zeros((n_classes, n_features));
        let mut covariances = Vec::with_capacity(n_classes);
        let mut class_counts = Array1::zeros(n_classes);

        for (i, &class) in classes.iter().enumerate() {
            let class_mask: Vec<bool> = y.iter().map(|&yi| yi == class).collect();
            let class_samples: Vec<ArrayView1<Float>> = x
                .axis_iter(Axis(0))
                .enumerate()
                .filter(|(j, _)| class_mask[*j])
                .map(|(_, sample)| sample)
                .collect();

            let count = class_samples.len();
            class_counts[i] = count as Float;

            if count == 0 {
                return Err(SklearsError::InvalidInput(format!(
                    "Class {} has no samples",
                    class
                )));
            }

            // Calculate class mean and covariance (robust or standard)
            let (class_mean, mut cov) = if self.config.robust && self.config.robust_method == "mcd"
            {
                // Convert class samples to matrix for robust estimation
                let class_data: Vec<Vec<Float>> =
                    class_samples.iter().map(|sample| sample.to_vec()).collect();

                let class_matrix =
                    Array2::from_shape_fn((class_data.len(), n_features), |(i, j)| {
                        class_data[i][j]
                    });

                // Use robust estimation
                match self.robust_covariance_mcd(&class_matrix) {
                    Ok((robust_mean, robust_cov, _weights)) => (robust_mean, robust_cov),
                    Err(_) => {
                        // Fall back to standard estimation if robust fails
                        let mut class_mean = Array1::zeros(n_features);
                        for sample in &class_samples {
                            for j in 0..n_features {
                                class_mean[j] += sample[j];
                            }
                        }
                        class_mean /= count as Float;

                        let mut cov = Array2::zeros((n_features, n_features));
                        if self.config.diagonal_covariance {
                            // Diagonal covariance: only compute diagonal elements
                            for sample in &class_samples {
                                let diff = sample - &class_mean;
                                for j in 0..n_features {
                                    cov[[j, j]] += diff[j] * diff[j];
                                }
                            }
                        } else {
                            // Full covariance matrix
                            for sample in &class_samples {
                                let diff = sample - &class_mean;
                                for j in 0..n_features {
                                    for k in 0..n_features {
                                        cov[[j, k]] += diff[j] * diff[k];
                                    }
                                }
                            }
                        }
                        if count > 1 {
                            cov /= (count - 1) as Float;
                        }
                        (class_mean, cov)
                    }
                }
            } else {
                // Standard estimation
                let mut class_mean = Array1::zeros(n_features);
                for sample in &class_samples {
                    for j in 0..n_features {
                        class_mean[j] += sample[j];
                    }
                }
                class_mean /= count as Float;

                let mut cov = Array2::zeros((n_features, n_features));
                if self.config.diagonal_covariance {
                    // Diagonal covariance: only compute diagonal elements
                    for sample in &class_samples {
                        let diff = sample - &class_mean;
                        for j in 0..n_features {
                            cov[[j, j]] += diff[j] * diff[j];
                        }
                    }
                } else {
                    // Full covariance matrix
                    for sample in &class_samples {
                        let diff = sample - &class_mean;
                        for j in 0..n_features {
                            for k in 0..n_features {
                                cov[[j, k]] += diff[j] * diff[k];
                            }
                        }
                    }
                }
                if count > 1 {
                    cov /= (count - 1) as Float;
                }
                (class_mean, cov)
            };

            means.row_mut(i).assign(&class_mean);

            // Add regularization
            if self.config.reg_param > 0.0 {
                for j in 0..n_features {
                    cov[[j, j]] += self.config.reg_param;
                }
            }

            covariances.push(cov);
        }

        // Calculate priors
        let priors = if let Some(ref p) = self.config.priors {
            p.clone()
        } else {
            &class_counts / n_samples as Float
        };

        let store_covariance = self.config.store_covariance;
        Ok(QuadraticDiscriminantAnalysis {
            config: self.config,
            state: PhantomData,
            classes_: Some(Array1::from(classes)),
            means_: Some(means),
            covariances_: if store_covariance {
                Some(covariances)
            } else {
                Some(covariances) // QDA always needs covariances for prediction
            },
            priors_: Some(priors),
            n_features_: Some(n_features),
        })
    }
}

impl QuadraticDiscriminantAnalysis<Trained> {
    /// Get the classes
    pub fn classes(&self) -> &Array1<i32> {
        self.classes_.as_ref().expect("Model is trained")
    }

    /// Get the means
    pub fn means(&self) -> &Array2<Float> {
        self.means_.as_ref().expect("Model is trained")
    }

    /// Get the priors
    pub fn priors(&self) -> &Array1<Float> {
        self.priors_.as_ref().expect("Model is trained")
    }

    /// Get the covariance matrices
    pub fn covariances(&self) -> &Vec<Array2<Float>> {
        self.covariances_.as_ref().expect("Model is trained")
    }
}

impl Predict<Array2<Float>, Array1<i32>> for QuadraticDiscriminantAnalysis<Trained> {
    fn predict(&self, x: &Array2<Float>) -> Result<Array1<i32>> {
        let probas = self.predict_proba(x)?;
        let classes = self.classes_.as_ref().expect("Model is trained");

        let predictions: Vec<i32> = probas
            .axis_iter(Axis(0))
            .map(|row| {
                let max_idx = row
                    .iter()
                    .enumerate()
                    .max_by(|a, b| a.1.partial_cmp(b.1).unwrap_or(std::cmp::Ordering::Equal))
                    .unwrap()
                    .0;
                classes[max_idx]
            })
            .collect();

        Ok(Array1::from(predictions))
    }
}

impl PredictProba<Array2<Float>, Array2<Float>> for QuadraticDiscriminantAnalysis<Trained> {
    fn predict_proba(&self, x: &Array2<Float>) -> Result<Array2<Float>> {
        let n_features = self.n_features_.expect("Model is trained");
        validate::check_n_features(x, n_features)?;

        let means = self.means_.as_ref().expect("Model is trained");
        let covariances = self.covariances_.as_ref().expect("Model is trained");
        let priors = self.priors_.as_ref().expect("Model is trained");
        let n_classes = means.nrows();
        let n_samples = x.nrows();

        let mut log_likelihoods = Array2::zeros((n_samples, n_classes));

        for i in 0..n_classes {
            let class_mean = means.row(i);
            let class_cov = &covariances[i];

            // Calculate log determinant
            let log_det = if self.config.diagonal_covariance {
                // For diagonal matrices, determinant is product of diagonal elements
                class_cov.diag().iter().map(|&x| x.ln()).sum::<Float>()
            } else {
                // For full matrices, use simplified calculation (product of diagonal elements)
                // This is an approximation - in practice, you'd use proper determinant calculation
                class_cov.diag().iter().product::<Float>().ln()
            };

            for j in 0..n_samples {
                let sample = x.row(j);
                let diff = &sample - &class_mean;

                // Quadratic form calculation
                let quad_form = if self.config.diagonal_covariance {
                    // For diagonal covariance: (x-μ)ᵀ Σ⁻¹ (x-μ) = Σᵢ (xᵢ-μᵢ)²/σᵢ²
                    let mut sum = 0.0;
                    for k in 0..n_features {
                        sum += diff[k] * diff[k] / class_cov[[k, k]];
                    }
                    sum
                } else {
                    // For full covariance: use simplified diagonal approximation
                    // In practice, you'd use proper matrix inverse
                    let mut sum = 0.0;
                    for k in 0..n_features {
                        sum += diff[k] * diff[k] / class_cov[[k, k]];
                    }
                    sum
                };

                log_likelihoods[[j, i]] = priors[i].ln() - 0.5 * (log_det + quad_form);
            }
        }

        // Convert log-likelihoods to probabilities using softmax
        let mut probabilities = Array2::zeros((n_samples, n_classes));
        for i in 0..n_samples {
            let row = log_likelihoods.row(i);
            let max_log_like = row.iter().fold(Float::NEG_INFINITY, |a, &b| a.max(b));

            let exp_likes: Vec<Float> = row.iter().map(|&x| (x - max_log_like).exp()).collect();
            let sum_exp: Float = exp_likes.iter().sum();

            for j in 0..n_classes {
                probabilities[[i, j]] = exp_likes[j] / sum_exp;
            }
        }

        Ok(probabilities)
    }
}
