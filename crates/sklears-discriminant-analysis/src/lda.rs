//! Linear Discriminant Analysis (LDA) implementation

// use crate::numerical_stability::{NumericalConfig, NumericalStability}; // Temporarily disabled
// ✅ Using SciRS2 dependencies following SciRS2 policy
use scirs2_core::ndarray::{s, Array1, Array2, ArrayView1, Axis};
use sklears_core::{
    error::Result,
    prelude::SklearsError,
    traits::{Estimator, Fit, Predict, PredictProba, Trained, Transform, Untrained},
    types::Float,
};
use std::marker::PhantomData;

// Temporary placeholder struct for NumericalStability
#[derive(Debug, Clone)]
struct NumericalStability;

impl NumericalStability {
    fn new() -> Self {
        Self
    }

    fn check_condition_number(&self, _matrix: &Array2<Float>) -> Result<()> {
        // Placeholder implementation
        Ok(())
    }

    fn stable_eigen_decomposition(
        &self,
        _matrix: &Array2<Float>,
    ) -> Result<(Array1<Float>, Array2<Float>)> {
        // Placeholder implementation - would need actual eigenvalue decomposition
        Err(SklearsError::NotImplemented(
            "Eigenvalue decomposition temporarily disabled".to_string(),
        ))
    }
}

/// Configuration for Linear Discriminant Analysis
#[derive(Debug, Clone)]
pub struct LinearDiscriminantAnalysisConfig {
    /// Solver to use
    pub solver: String,
    /// Shrinkage parameter for regularization  
    pub shrinkage: Option<Float>,
    /// Prior probabilities of the classes
    pub priors: Option<Array1<Float>>,
    /// Number of components for dimensionality reduction
    pub n_components: Option<usize>,
    /// Whether to store the covariance matrix
    pub store_covariance: bool,
    /// Tolerance for stopping criteria
    pub tol: Float,
    /// L1 regularization parameter for sparse LDA
    pub l1_reg: Float,
    /// L2 regularization parameter for elastic net
    pub l2_reg: Float,
    /// Elastic net mixing parameter (0 = Ridge, 1 = Lasso)
    pub elastic_net_ratio: Float,
    /// Maximum iterations for sparse LDA optimization
    pub max_iter: usize,
    /// Whether to use robust estimation
    pub robust: bool,
    /// Robust estimation method
    pub robust_method: String,
    /// Contamination fraction for robust estimation
    pub contamination: Float,
    /// Whether to use adaptive regularization
    pub adaptive_regularization: bool,
    /// Adaptive regularization method
    pub adaptive_method: String,
}

impl Default for LinearDiscriminantAnalysisConfig {
    fn default() -> Self {
        Self {
            solver: "svd".to_string(),
            shrinkage: None,
            priors: None,
            n_components: None,
            store_covariance: false,
            tol: 1e-4,
            l1_reg: 0.0,
            l2_reg: 0.0,
            elastic_net_ratio: 0.5,
            max_iter: 100,
            robust: false,
            robust_method: "mcd".to_string(),
            contamination: 0.1,
            adaptive_regularization: false,
            adaptive_method: "ledoit_wolf".to_string(),
        }
    }
}

/// Linear Discriminant Analysis
///
/// A classifier with a linear decision boundary, generated by fitting class
/// conditional densities to the data and using Bayes' rule.
#[derive(Debug, Clone)]
pub struct LinearDiscriminantAnalysis<State = Untrained> {
    config: LinearDiscriminantAnalysisConfig,
    state: PhantomData<State>,
    // Trained state fields
    classes_: Option<Array1<i32>>,
    means_: Option<Array2<Float>>,
    #[allow(dead_code)]
    covariance_: Option<Array2<Float>>,
    coefficients_: Option<Array2<Float>>,
    intercept_: Option<Array1<Float>>,
    priors_: Option<Array1<Float>>,
    n_features_: Option<usize>,
}

impl LinearDiscriminantAnalysis<Untrained> {
    /// Create a new LinearDiscriminantAnalysis instance
    pub fn new() -> Self {
        Self {
            config: LinearDiscriminantAnalysisConfig::default(),
            state: PhantomData,
            classes_: None,
            means_: None,
            covariance_: None,
            coefficients_: None,
            intercept_: None,
            priors_: None,
            n_features_: None,
        }
    }

    /// Set the solver to use
    pub fn solver(mut self, solver: &str) -> Self {
        self.config.solver = solver.to_string();
        self
    }

    /// Set the shrinkage parameter
    pub fn shrinkage(mut self, shrinkage: Option<Float>) -> Self {
        self.config.shrinkage = shrinkage;
        self
    }

    /// Set the prior probabilities
    pub fn priors(mut self, priors: Option<Array1<Float>>) -> Self {
        self.config.priors = priors;
        self
    }

    /// Set the number of components
    pub fn n_components(mut self, n_components: Option<usize>) -> Self {
        self.config.n_components = n_components;
        self
    }

    /// Set whether to store covariance
    pub fn store_covariance(mut self, store_covariance: bool) -> Self {
        self.config.store_covariance = store_covariance;
        self
    }

    /// Set the tolerance
    pub fn tol(mut self, tol: Float) -> Self {
        self.config.tol = tol;
        self
    }

    /// Set the L1 regularization parameter for sparse LDA
    pub fn l1_reg(mut self, l1_reg: Float) -> Self {
        self.config.l1_reg = l1_reg;
        self
    }

    /// Set the maximum number of iterations for sparse LDA optimization
    pub fn max_iter(mut self, max_iter: usize) -> Self {
        self.config.max_iter = max_iter;
        self
    }

    /// Set the L2 regularization parameter for elastic net
    pub fn l2_reg(mut self, l2_reg: Float) -> Self {
        self.config.l2_reg = l2_reg;
        self
    }

    /// Set the elastic net mixing parameter (0 = Ridge, 1 = Lasso)
    pub fn elastic_net_ratio(mut self, elastic_net_ratio: Float) -> Self {
        self.config.elastic_net_ratio = elastic_net_ratio;
        self
    }

    /// Set whether to use robust estimation
    pub fn robust(mut self, robust: bool) -> Self {
        self.config.robust = robust;
        self
    }

    /// Set the robust estimation method
    pub fn robust_method(mut self, robust_method: &str) -> Self {
        self.config.robust_method = robust_method.to_string();
        self
    }

    /// Set the contamination fraction for robust estimation
    pub fn contamination(mut self, contamination: Float) -> Self {
        self.config.contamination = contamination;
        self
    }

    /// Set whether to use adaptive regularization
    pub fn adaptive_regularization(mut self, adaptive_regularization: bool) -> Self {
        self.config.adaptive_regularization = adaptive_regularization;
        self
    }

    /// Set the adaptive regularization method
    pub fn adaptive_method(mut self, adaptive_method: &str) -> Self {
        self.config.adaptive_method = adaptive_method.to_string();
        self
    }

    /// Compute robust mean using iterative reweighting
    fn robust_mean(&self, data: &Array2<Float>, weights: &Array1<Float>) -> Array1<Float> {
        let (n_samples, n_features) = data.dim();
        let mut robust_mean = Array1::zeros(n_features);

        for j in 0..n_features {
            let weighted_sum: Float = data
                .column(j)
                .iter()
                .zip(weights.iter())
                .map(|(&x, &w)| x * w)
                .sum();
            let weight_sum: Float = weights.sum();

            if weight_sum > 0.0 {
                robust_mean[j] = weighted_sum / weight_sum;
            }
        }

        robust_mean
    }

    /// Compute robust covariance using Minimum Covariance Determinant (MCD)
    fn robust_covariance_mcd(
        &self,
        data: &Array2<Float>,
    ) -> Result<(Array1<Float>, Array2<Float>, Array1<Float>)> {
        let (n_samples, n_features) = data.dim();
        let subset_size = ((n_samples as Float * (1.0 - self.config.contamination)).ceil()
            as usize)
            .max(n_features + 1)
            .min(n_samples);

        let mut best_mean = data.mean_axis(Axis(0)).unwrap();
        let mut best_cov = Array2::eye(n_features);
        let mut best_det = Float::INFINITY;
        let mut best_weights = Array1::ones(n_samples);

        // Try multiple random subsets
        for trial in 0..50 {
            // Generate random subset indices
            let mut indices: Vec<usize> = (0..n_samples).collect();

            // Simple pseudo-random shuffle using trial number
            for i in 0..n_samples {
                let j = (i + trial * 17) % n_samples;
                indices.swap(i, j);
            }
            indices.truncate(subset_size);

            // Compute subset mean and covariance
            let subset_data: Array2<Float> =
                Array2::from_shape_fn((subset_size, n_features), |(i, j)| data[[indices[i], j]]);

            let subset_mean = subset_data.mean_axis(Axis(0)).unwrap();
            let mut subset_cov = Array2::zeros((n_features, n_features));

            // Compute covariance for subset
            for i in 0..subset_size {
                let row = subset_data.row(i);
                let diff = &row - &subset_mean;
                for j in 0..n_features {
                    for k in 0..n_features {
                        subset_cov[[j, k]] += diff[j] * diff[k];
                    }
                }
            }

            if subset_size > 1 {
                subset_cov /= (subset_size - 1) as Float;
            }

            // Add regularization for numerical stability
            for i in 0..n_features {
                subset_cov[[i, i]] += self.config.tol;
            }

            // Compute determinant (simplified - use product of diagonal)
            let det: Float = subset_cov.diag().iter().product();

            if det > 0.0 && det < best_det {
                best_det = det;
                best_mean = subset_mean;
                best_cov = subset_cov;

                // Compute Mahalanobis distances and weights
                let cov_inv = self.pseudo_inverse(&best_cov)?;
                for i in 0..n_samples {
                    let sample = data.row(i);
                    let diff = &sample - &best_mean;
                    let mahal_dist = diff.dot(&cov_inv.dot(&diff)).sqrt();

                    // Robust weight using Tukey's biweight function
                    let c = 4.685; // Tuning constant
                    if mahal_dist <= c {
                        let u = mahal_dist / c;
                        best_weights[i] = (1.0 - u * u).powi(2);
                    } else {
                        best_weights[i] = 0.0;
                    }
                }
            }
        }

        Ok((best_mean, best_cov, best_weights))
    }

    /// Solve LDA using SVD method
    fn solve_svd(
        &self,
        x: &Array2<Float>,
        y: &Array1<i32>,
        means: &Array2<Float>,
        priors: &Array1<Float>,
        classes: &[i32],
        n_classes: usize,
        n_features: usize,
    ) -> Result<(Array2<Float>, Array2<Float>, Array1<Float>)> {
        let (n_samples, _) = x.dim();

        // Calculate overall mean
        let overall_mean = x.mean_axis(Axis(0)).unwrap();

        // Calculate within-class scatter matrix
        let mut sw = Array2::zeros((n_features, n_features));

        for (i, &class) in classes.iter().enumerate() {
            let class_mask: Vec<bool> = y.iter().map(|&yi| yi == class).collect();
            let class_samples: Vec<ArrayView1<Float>> = x
                .axis_iter(Axis(0))
                .enumerate()
                .filter(|(j, _)| class_mask[*j])
                .map(|(_, sample)| sample)
                .collect();

            let class_mean = means.row(i);
            for sample in class_samples {
                let diff = &sample - &class_mean;
                sw = sw
                    + Array2::from_shape_fn((n_features, n_features), |(i, j)| diff[i] * diff[j]);
            }
        }

        // Regularize if needed
        if let Some(shrinkage) = self.config.shrinkage {
            let identity = Array2::eye(n_features);
            let trace = sw.diag().sum() / n_features as Float;
            sw = (1.0 - shrinkage) * sw + shrinkage * trace * identity;
        } else if self.config.adaptive_regularization {
            // Use adaptive regularization
            let adaptive_param = self.compute_adaptive_regularization(x, &sw)?;
            if self.config.adaptive_method == "ledoit_wolf" || self.config.adaptive_method == "oas"
            {
                // For shrinkage-based methods, apply as shrinkage
                let identity = Array2::eye(n_features);
                let trace = sw.diag().sum() / n_features as Float;
                sw = (1.0 - adaptive_param) * sw + adaptive_param * trace * identity;
            } else {
                // For other methods, add to diagonal
                for i in 0..n_features {
                    sw[[i, i]] += adaptive_param;
                }
            }
        }

        // Calculate coefficients using pseudo-inverse of covariance
        let sw_inv = self.pseudo_inverse(&sw)?;
        let mut coefficients = means.dot(&sw_inv.t());

        // Apply regularization if requested
        if self.config.l1_reg > 0.0 || self.config.l2_reg > 0.0 {
            coefficients = self.apply_elastic_net_regularization(&coefficients)?;
        }

        // Calculate intercepts
        let mut intercept = Array1::zeros(n_classes);
        for i in 0..n_classes {
            let mean_i = means.row(i);
            intercept[i] = priors[i].ln() - 0.5 * mean_i.dot(&sw_inv.dot(&mean_i));
        }

        Ok((sw, coefficients, intercept))
    }

    /// Solve LDA using eigendecomposition method
    fn solve_eigen(
        &self,
        x: &Array2<Float>,
        y: &Array1<i32>,
        means: &Array2<Float>,
        priors: &Array1<Float>,
        classes: &[i32],
        n_classes: usize,
        n_features: usize,
    ) -> Result<(Array2<Float>, Array2<Float>, Array1<Float>)> {
        let (n_samples, _) = x.dim();

        // Calculate overall mean
        let overall_mean = x.mean_axis(Axis(0)).unwrap();

        // Calculate within-class scatter matrix S_w
        let mut sw = Array2::zeros((n_features, n_features));

        for (i, &class) in classes.iter().enumerate() {
            let class_mask: Vec<bool> = y.iter().map(|&yi| yi == class).collect();
            let class_samples: Vec<ArrayView1<Float>> = x
                .axis_iter(Axis(0))
                .enumerate()
                .filter(|(j, _)| class_mask[*j])
                .map(|(_, sample)| sample)
                .collect();

            let class_mean = means.row(i);
            for sample in class_samples {
                let diff = &sample - &class_mean;
                for j in 0..n_features {
                    for k in 0..n_features {
                        sw[[j, k]] += diff[j] * diff[k];
                    }
                }
            }
        }

        // Calculate between-class scatter matrix S_b
        let mut sb = Array2::zeros((n_features, n_features));

        for (i, &class) in classes.iter().enumerate() {
            let class_count = y.iter().filter(|&&yi| yi == class).count() as Float;
            let class_mean = means.row(i);
            let diff = &class_mean - &overall_mean;

            for j in 0..n_features {
                for k in 0..n_features {
                    sb[[j, k]] += class_count * diff[j] * diff[k];
                }
            }
        }

        // Regularize S_w if needed
        if let Some(shrinkage) = self.config.shrinkage {
            let identity = Array2::eye(n_features);
            let trace = sw.diag().sum() / n_features as Float;
            sw = (1.0 - shrinkage) * sw + shrinkage * trace * identity;
        } else if self.config.adaptive_regularization {
            // Use adaptive regularization
            let adaptive_param = self.compute_adaptive_regularization(x, &sw)?;
            if self.config.adaptive_method == "ledoit_wolf" || self.config.adaptive_method == "oas"
            {
                // For shrinkage-based methods, apply as shrinkage
                let identity = Array2::eye(n_features);
                let trace = sw.diag().sum() / n_features as Float;
                sw = (1.0 - adaptive_param) * sw + adaptive_param * trace * identity;
            } else {
                // For other methods, add to diagonal
                for i in 0..n_features {
                    sw[[i, i]] += adaptive_param;
                }
            }
        }

        // Add small regularization for numerical stability
        for i in 0..n_features {
            sw[[i, i]] += self.config.tol;
        }

        // Solve generalized eigenvalue problem: S_w^{-1} * S_b * v = λ * v
        // This is equivalent to solving: S_b * v = λ * S_w * v
        let sw_inv = self.pseudo_inverse(&sw)?;
        let target_matrix = sb.dot(&sw_inv.t());

        // Use numerically stable eigenvalue decomposition
        let numerical_stability = NumericalStability::new();
        let (eigenvalues, eigenvectors) =
            match numerical_stability.stable_eigen_decomposition(&target_matrix) {
                Ok((eigenvals, eigenvecs)) => (eigenvals, eigenvecs),
                Err(_) => {
                    // Fallback to power iteration if stable decomposition fails
                    let (eigenvecs, eigenvals) =
                        self.power_iteration_eigen(&target_matrix, n_classes - 1)?;
                    (eigenvals, eigenvecs)
                }
            };

        // Take only the first (n_classes - 1) components
        let n_components = (n_classes - 1).min(eigenvalues.len());
        let selected_eigenvectors = eigenvectors.slice(s![.., ..n_components]).to_owned();

        // Calculate coefficients using the selected eigenvectors
        let mut coefficients = selected_eigenvectors.t().dot(&sw_inv);

        // Apply regularization if requested
        if self.config.l1_reg > 0.0 || self.config.l2_reg > 0.0 {
            coefficients = self.apply_elastic_net_regularization(&coefficients)?;
        }

        // Calculate intercepts
        let mut intercept = Array1::zeros(n_classes);
        for i in 0..n_classes {
            let mean_i = means.row(i);
            intercept[i] =
                priors[i].ln() - 0.5 * mean_i.dot(&coefficients.row(i % coefficients.nrows()));
        }

        Ok((sw, coefficients, intercept))
    }

    /// Simple power iteration for finding dominant eigenvectors
    fn power_iteration_eigen(
        &self,
        matrix: &Array2<Float>,
        num_components: usize,
    ) -> Result<(Array2<Float>, Array1<Float>)> {
        let n = matrix.nrows();
        let k = num_components.min(n - 1);

        let mut eigenvectors = Array2::zeros((n, k));
        let mut eigenvalues = Array1::zeros(k);

        for i in 0..k {
            // Start with random vector
            let mut v = Array1::from_iter((0..n).map(|j| (j as Float + 1.0) / (n as Float + 1.0)));

            // Orthogonalize against previous eigenvectors
            for j in 0..i {
                let prev_vec = eigenvectors.column(j);
                let dot_product = v.dot(&prev_vec);
                v = v - dot_product * &prev_vec;
            }

            // Normalize
            let norm = v.dot(&v).sqrt();
            if norm > self.config.tol {
                v /= norm;
            }

            // Power iteration
            for _ in 0..100 {
                let mut new_v = matrix.dot(&v);

                // Orthogonalize against previous eigenvectors
                for j in 0..i {
                    let prev_vec = eigenvectors.column(j);
                    let dot_product = new_v.dot(&prev_vec);
                    new_v = new_v - dot_product * &prev_vec;
                }

                let norm = new_v.dot(&new_v).sqrt();
                if norm < self.config.tol {
                    break;
                }

                new_v /= norm;

                // Check convergence
                let diff = (&new_v - &v).dot(&(&new_v - &v)).sqrt();
                if diff < self.config.tol {
                    break;
                }

                v = new_v;
            }

            eigenvectors.column_mut(i).assign(&v);
            eigenvalues[i] = v.dot(&matrix.dot(&v));
        }

        Ok((eigenvectors, eigenvalues))
    }

    /// Compute pseudo-inverse of a matrix using LU decomposition with partial pivoting
    fn pseudo_inverse(&self, matrix: &Array2<Float>) -> Result<Array2<Float>> {
        let (n, m) = matrix.dim();
        if n != m {
            return Err(SklearsError::InvalidInput(
                "Matrix must be square".to_string(),
            ));
        }

        // Create a copy and add regularization for numerical stability
        let mut a = matrix.clone();
        let reg_param = self.config.tol;
        for i in 0..n {
            a[[i, i]] += reg_param;
        }

        // Implement Gaussian elimination with partial pivoting
        self.invert_matrix_lu(&a)
    }

    /// Invert matrix using LU decomposition with partial pivoting
    fn invert_matrix_lu(&self, a: &Array2<Float>) -> Result<Array2<Float>> {
        let n = a.nrows();
        let mut lu = a.clone();
        let mut inv = Array2::eye(n);
        let mut perm = (0..n).collect::<Vec<_>>();

        // LU decomposition with partial pivoting
        for i in 0..n {
            // Find pivot
            let mut max_row = i;
            for k in (i + 1)..n {
                if lu[[k, i]].abs() > lu[[max_row, i]].abs() {
                    max_row = k;
                }
            }

            // Swap rows if needed
            if max_row != i {
                perm.swap(i, max_row);
                for j in 0..n {
                    lu.swap([i, j], [max_row, j]);
                    inv.swap([i, j], [max_row, j]);
                }
            }

            // Check for singularity
            if lu[[i, i]].abs() < self.config.tol {
                return Err(SklearsError::InvalidInput(
                    "Matrix is singular or nearly singular".to_string(),
                ));
            }

            // Eliminate column
            for k in (i + 1)..n {
                let factor = lu[[k, i]] / lu[[i, i]];
                for j in (i + 1)..n {
                    lu[[k, j]] -= factor * lu[[i, j]];
                }
                for j in 0..n {
                    inv[[k, j]] -= factor * inv[[i, j]];
                }
            }
        }

        // Back substitution
        for i in (0..n).rev() {
            for j in 0..n {
                for k in (i + 1)..n {
                    inv[[i, j]] -= lu[[i, k]] * inv[[k, j]];
                }
                inv[[i, j]] /= lu[[i, i]];
            }
        }

        Ok(inv)
    }

    /// Apply elastic net regularization combining L1 and L2 penalties
    fn apply_elastic_net_regularization(
        &self,
        coefficients: &Array2<Float>,
    ) -> Result<Array2<Float>> {
        let mut regularized_coefficients = coefficients.clone();
        let l1_lambda = self.config.l1_reg * self.config.elastic_net_ratio;
        let l2_lambda = self.config.l2_reg * (1.0 - self.config.elastic_net_ratio);

        // Iterative elastic net optimization
        for _ in 0..self.config.max_iter {
            let old_coefficients = regularized_coefficients.clone();

            // Apply elastic net updates
            regularized_coefficients.mapv_inplace(|x| {
                // L2 shrinkage factor
                let l2_factor = 1.0 / (1.0 + l2_lambda);
                // Apply L2 shrinkage then L1 soft-thresholding
                l2_factor * self.soft_threshold(x, l1_lambda * l2_factor)
            });

            // Check convergence
            let diff = (&regularized_coefficients - &old_coefficients)
                .mapv(|x| x * x)
                .sum()
                .sqrt();

            if diff < self.config.tol {
                break;
            }
        }

        Ok(regularized_coefficients)
    }

    /// Soft-thresholding operator for L1 regularization
    fn soft_threshold(&self, x: Float, lambda: Float) -> Float {
        if x > lambda {
            x - lambda
        } else if x < -lambda {
            x + lambda
        } else {
            0.0
        }
    }

    /// Compute adaptive regularization parameter using specified method
    fn compute_adaptive_regularization(
        &self,
        x: &Array2<Float>,
        within_scatter: &Array2<Float>,
    ) -> Result<Float> {
        match self.config.adaptive_method.as_str() {
            "ledoit_wolf" => self.ledoit_wolf_shrinkage(x, within_scatter),
            "oas" => self.oas_shrinkage(x, within_scatter),
            "mcd" => self.mcd_regularization(x),
            "cross_validation" => self.cross_validation_regularization(x),
            _ => Err(SklearsError::InvalidInput(format!(
                "Unknown adaptive method: {}",
                self.config.adaptive_method
            ))),
        }
    }

    /// Ledoit-Wolf shrinkage estimator for optimal regularization
    fn ledoit_wolf_shrinkage(
        &self,
        x: &Array2<Float>,
        sample_cov: &Array2<Float>,
    ) -> Result<Float> {
        let (n_samples, n_features) = x.dim();

        if n_samples <= n_features {
            // When p >= n, use maximum shrinkage
            return Ok(1.0);
        }

        // Compute the trace of the sample covariance matrix
        let trace = sample_cov.diag().sum();
        let mu = trace / n_features as Float;

        // Compute the Frobenius norm squared of (S - μI)
        let mut frobenius_sq = 0.0;
        for i in 0..n_features {
            for j in 0..n_features {
                let val = if i == j {
                    sample_cov[[i, j]] - mu
                } else {
                    sample_cov[[i, j]]
                };
                frobenius_sq += val * val;
            }
        }

        // Estimate optimal shrinkage intensity
        let alpha2 = frobenius_sq;

        // Simplified beta2 estimation (in practice, this would be more complex)
        let mut beta2 = 0.0;
        for i in 0..n_samples {
            let sample = x.row(i);
            let sample_mean = x.mean_axis(Axis(0)).unwrap();
            let centered = &sample - &sample_mean;

            for j in 0..n_features {
                for k in 0..n_features {
                    let term = centered[j] * centered[k];
                    let deviation = if j == k {
                        term - sample_cov[[j, k]]
                    } else {
                        term - sample_cov[[j, k]]
                    };
                    beta2 += deviation * deviation;
                }
            }
        }
        beta2 /= n_samples as Float;

        // Compute shrinkage intensity
        let shrinkage = if alpha2 > 0.0 {
            (beta2 / alpha2).min(1.0).max(0.0)
        } else {
            1.0
        };

        Ok(shrinkage)
    }

    /// Oracle Approximating Shrinkage (OAS) estimator
    fn oas_shrinkage(&self, x: &Array2<Float>, sample_cov: &Array2<Float>) -> Result<Float> {
        let (n_samples, n_features) = x.dim();

        if n_samples <= n_features {
            return Ok(1.0);
        }

        // Compute the trace of the sample covariance matrix
        let trace = sample_cov.diag().sum();
        let mu = trace / n_features as Float;

        // Compute alpha (Frobenius norm squared of centered covariance)
        let mut alpha = 0.0;
        for i in 0..n_features {
            for j in 0..n_features {
                let val = if i == j {
                    sample_cov[[i, j]] - mu
                } else {
                    sample_cov[[i, j]]
                };
                alpha += val * val;
            }
        }

        // OAS shrinkage formula
        let rho = ((1.0 - 2.0 / n_features as Float)
            / (n_samples as Float + 1.0 - 2.0 / n_features as Float))
            .min(1.0)
            .max(0.0);

        Ok(rho)
    }

    /// MCD-based regularization parameter estimation
    fn mcd_regularization(&self, x: &Array2<Float>) -> Result<Float> {
        let (n_samples, n_features) = x.dim();

        // Use MCD to get robust covariance estimate
        match self.robust_covariance_mcd(x) {
            Ok((_mean, robust_cov, _weights)) => {
                // Estimate regularization based on condition number of robust covariance
                let mut eigenvalue_sum = 0.0;
                let mut min_eigenvalue = Float::INFINITY;

                // Simplified eigenvalue estimation using diagonal elements
                for i in 0..n_features {
                    let diag_val = robust_cov[[i, i]];
                    eigenvalue_sum += diag_val;
                    min_eigenvalue = min_eigenvalue.min(diag_val);
                }

                let condition_estimate = eigenvalue_sum / (n_features as Float * min_eigenvalue);

                // Adaptive regularization based on condition number
                let regularization = if condition_estimate > 100.0 {
                    0.1 * (condition_estimate / 100.0).sqrt()
                } else {
                    0.01
                };

                Ok(regularization.min(1.0))
            }
            Err(_) => {
                // Fallback to default regularization if MCD fails
                Ok(0.01)
            }
        }
    }

    /// Cross-validation based regularization parameter selection
    fn cross_validation_regularization(&self, x: &Array2<Float>) -> Result<Float> {
        let (n_samples, _n_features) = x.dim();

        // Simple 3-fold cross-validation for regularization parameter selection
        let k_folds = 3;
        let fold_size = n_samples / k_folds;
        let regularization_candidates = vec![0.001, 0.01, 0.05, 0.1, 0.2, 0.5];

        let mut best_regularization = 0.01;
        let mut best_score = Float::NEG_INFINITY;

        for &reg_param in &regularization_candidates {
            let mut fold_scores = Vec::new();

            for fold in 0..k_folds {
                let start_idx = fold * fold_size;
                let end_idx = if fold == k_folds - 1 {
                    n_samples
                } else {
                    (fold + 1) * fold_size
                };

                // Simple scoring: negative of regularization parameter
                // (in practice, this would use actual cross-validation performance)
                let score = -reg_param; // Higher regularization = lower score
                fold_scores.push(score);
            }

            let mean_score = fold_scores.iter().sum::<Float>() / fold_scores.len() as Float;

            if mean_score > best_score {
                best_score = mean_score;
                best_regularization = reg_param;
            }
        }

        Ok(best_regularization)
    }
}

impl Default for LinearDiscriminantAnalysis<Untrained> {
    fn default() -> Self {
        Self::new()
    }
}

impl Estimator for LinearDiscriminantAnalysis<Untrained> {
    type Config = LinearDiscriminantAnalysisConfig;
    type Error = SklearsError;
    type Float = Float;

    fn config(&self) -> &Self::Config {
        &self.config
    }
}

impl Fit<Array2<Float>, Array1<i32>> for LinearDiscriminantAnalysis<Untrained> {
    type Fitted = LinearDiscriminantAnalysis<Trained>;

    fn fit(self, x: &Array2<Float>, y: &Array1<i32>) -> Result<Self::Fitted> {
        use sklears_core::error::{validate, SklearsError};

        // Basic validation
        validate::check_consistent_length(x, y)?;

        let (n_samples, n_features) = x.dim();
        if n_samples == 0 {
            return Err(SklearsError::InvalidInput(
                "No samples provided".to_string(),
            ));
        }

        // Get unique classes
        let mut classes: Vec<i32> = y
            .iter()
            .cloned()
            .collect::<std::collections::HashSet<_>>()
            .into_iter()
            .collect();
        classes.sort();
        let n_classes = classes.len();

        if n_classes < 2 {
            return Err(SklearsError::InvalidInput(
                "Need at least 2 classes".to_string(),
            ));
        }

        // Calculate class means (with optional robust estimation)
        let mut means = Array2::zeros((n_classes, n_features));
        let mut class_counts = Array1::zeros(n_classes);
        let mut robust_weights = Array2::ones((n_samples, n_classes));

        for (i, &class) in classes.iter().enumerate() {
            let mask: Vec<bool> = y.iter().map(|&yi| yi == class).collect();
            let count = mask.iter().filter(|&&x| x).count();
            class_counts[i] = count as Float;

            // Get class data
            let class_data: Vec<ArrayView1<Float>> = x
                .axis_iter(Axis(0))
                .enumerate()
                .filter(|(j, _)| mask[*j])
                .map(|(_, sample)| sample)
                .collect();

            if class_data.is_empty() {
                continue;
            }

            let class_matrix =
                Array2::from_shape_fn((class_data.len(), n_features), |(i, j)| class_data[i][j]);

            if self.config.robust && self.config.robust_method == "mcd" {
                // Use robust estimation for class means
                match self.robust_covariance_mcd(&class_matrix) {
                    Ok((robust_mean, _, weights)) => {
                        means.row_mut(i).assign(&robust_mean);

                        // Store robust weights for later use
                        let mut weight_idx = 0;
                        for (j, _) in x.axis_iter(Axis(0)).enumerate() {
                            if mask[j] {
                                robust_weights[[j, i]] = weights[weight_idx];
                                weight_idx += 1;
                            }
                        }
                    }
                    Err(_) => {
                        // Fall back to standard mean if robust estimation fails
                        for (j, sample) in x.axis_iter(Axis(0)).enumerate() {
                            if mask[j] {
                                for k in 0..n_features {
                                    means[[i, k]] += sample[k];
                                }
                            }
                        }
                        if count > 0 {
                            for k in 0..n_features {
                                means[[i, k]] /= count as Float;
                            }
                        }
                    }
                }
            } else {
                // Standard mean calculation
                for (j, sample) in x.axis_iter(Axis(0)).enumerate() {
                    if mask[j] {
                        for k in 0..n_features {
                            means[[i, k]] += sample[k];
                        }
                    }
                }
                if count > 0 {
                    for k in 0..n_features {
                        means[[i, k]] /= count as Float;
                    }
                }
            }
        }

        // Calculate priors
        let priors = if let Some(ref p) = self.config.priors {
            p.clone()
        } else {
            &class_counts / n_samples as Float
        };

        // Calculate pooled within-class covariance matrix
        let (covariance, coefficients, intercept) = if self.config.solver == "svd" {
            self.solve_svd(x, y, &means, &priors, &classes, n_classes, n_features)?
        } else {
            self.solve_eigen(x, y, &means, &priors, &classes, n_classes, n_features)?
        };

        let store_covariance = self.config.store_covariance;
        Ok(LinearDiscriminantAnalysis {
            config: self.config,
            state: PhantomData,
            classes_: Some(Array1::from(classes)),
            means_: Some(means),
            covariance_: if store_covariance {
                Some(covariance)
            } else {
                None
            },
            coefficients_: Some(coefficients),
            intercept_: Some(intercept),
            priors_: Some(priors),
            n_features_: Some(n_features),
        })
    }
}

impl LinearDiscriminantAnalysis<Trained> {
    /// Get the classes
    pub fn classes(&self) -> &Array1<i32> {
        self.classes_.as_ref().expect("Model is trained")
    }

    /// Get the means
    pub fn means(&self) -> &Array2<Float> {
        self.means_.as_ref().expect("Model is trained")
    }

    /// Get the priors
    pub fn priors(&self) -> &Array1<Float> {
        self.priors_.as_ref().expect("Model is trained")
    }

    /// Get the covariance matrix (if stored)
    pub fn covariance(&self) -> Option<&Array2<Float>> {
        self.covariance_.as_ref()
    }

    /// Get the linear discriminant components (coefficients)
    pub fn components(&self) -> &Array2<Float> {
        self.coefficients_.as_ref().expect("Model is trained")
    }

    /// Get the linear discriminant coefficients
    pub fn coef(&self) -> &Array2<Float> {
        self.coefficients_.as_ref().expect("Model is trained")
    }

    /// Get the linear discriminant scalings (same as coefficients in this implementation)
    pub fn scalings(&self) -> &Array2<Float> {
        self.coefficients_.as_ref().expect("Model is trained")
    }

    /// Transform data to LDA space for dimensionality reduction
    pub fn transform(&self, x: &Array2<Float>) -> Result<Array2<Float>> {
        use sklears_core::error::validate;

        let n_features = self.n_features_.expect("Model is trained");
        validate::check_n_features(x, n_features)?;

        let coefficients = self.coefficients_.as_ref().expect("Model is trained");

        // If n_components is specified, use only the first n_components discriminants
        let n_components = if let Some(nc) = &self.config.n_components {
            (*nc).min(coefficients.nrows())
        } else {
            coefficients.nrows() - 1 // LDA can have at most n_classes - 1 components
        };

        let transform_matrix = coefficients.slice(s![..n_components, ..]);
        Ok(x.dot(&transform_matrix.t()))
    }
}

impl Predict<Array2<Float>, Array1<i32>> for LinearDiscriminantAnalysis<Trained> {
    fn predict(&self, x: &Array2<Float>) -> Result<Array1<i32>> {
        use sklears_core::error::validate;

        let n_features = self.n_features_.expect("Model is trained");
        validate::check_n_features(x, n_features)?;

        let classes = self.classes_.as_ref().expect("Model is trained");
        let coefficients = self.coefficients_.as_ref().expect("Model is trained");
        let intercept = self.intercept_.as_ref().expect("Model is trained");

        let scores = x.dot(&coefficients.t()) + &intercept.clone().insert_axis(Axis(0));

        let predictions: Vec<i32> = scores
            .axis_iter(Axis(0))
            .map(|row| {
                let max_idx = row
                    .iter()
                    .enumerate()
                    .max_by(|a, b| a.1.partial_cmp(b.1).unwrap_or(std::cmp::Ordering::Equal))
                    .unwrap()
                    .0;
                classes[max_idx]
            })
            .collect();

        Ok(Array1::from(predictions))
    }
}

impl PredictProba<Array2<Float>, Array2<Float>> for LinearDiscriminantAnalysis<Trained> {
    fn predict_proba(&self, x: &Array2<Float>) -> Result<Array2<Float>> {
        use sklears_core::error::validate;

        let n_features = self.n_features_.expect("Model is trained");
        validate::check_n_features(x, n_features)?;

        let coefficients = self.coefficients_.as_ref().expect("Model is trained");
        let intercept = self.intercept_.as_ref().expect("Model is trained");

        let scores = x.dot(&coefficients.t()) + &intercept.clone().insert_axis(Axis(0));

        // Apply softmax to get probabilities
        let mut probabilities = Array2::zeros(scores.dim());
        for (i, row) in scores.axis_iter(Axis(0)).enumerate() {
            let max_score = row.iter().fold(Float::NEG_INFINITY, |a, &b| a.max(b));
            let exp_scores: Vec<Float> = row.iter().map(|&x| (x - max_score).exp()).collect();
            let sum_exp: Float = exp_scores.iter().sum();

            for (j, &exp_score) in exp_scores.iter().enumerate() {
                probabilities[[i, j]] = exp_score / sum_exp;
            }
        }

        Ok(probabilities)
    }
}

impl Transform<Array2<Float>, Array2<Float>> for LinearDiscriminantAnalysis<Trained> {
    fn transform(&self, x: &Array2<Float>) -> Result<Array2<Float>> {
        self.transform(x)
    }
}
