//! Quadratic Discriminant Analysis (QDA)
//!
//! This module provides a comprehensive implementation of Quadratic Discriminant Analysis,
//! a powerful classification algorithm that assumes each class has its own covariance matrix,
//! allowing for quadratic decision boundaries.

use scirs2_core::ndarray::{Array1, Array2, ArrayView1, Axis, array};
use scirs2_core::random::{Random, rng};
use sklears_core::{
    error::{validate, Result as SklResult, SklearsError},
    traits::{Estimator, Fit, Predict, PredictProba, Trained, Transform, Untrained},
    types::Float,
};
use std::marker::PhantomData;

/// Configuration for Quadratic Discriminant Analysis
///
/// QDA assumes that each class has its own covariance matrix, allowing for
/// quadratic decision boundaries between classes. This makes it more flexible
/// than LDA but requires more parameters to estimate.
#[derive(Debug, Clone)]
pub struct QuadraticDiscriminantAnalysisConfig {
    /// Prior probabilities of the classes
    /// If None, priors are estimated from the training data
    pub priors: Option<Array1<Float>>,

    /// Regularization parameter for covariance matrices
    /// Adds this value to the diagonal of covariance matrices for numerical stability
    pub reg_param: Float,

    /// Whether to store the covariance matrices after fitting
    /// Note: QDA always needs covariances for prediction
    pub store_covariance: bool,

    /// Tolerance for stopping criteria and numerical stability
    pub tol: Float,

    /// Whether to use diagonal covariance matrices only
    /// Useful for high-dimensional data where full covariance estimation is unreliable
    pub diagonal_covariance: bool,

    /// Whether to use robust estimation methods
    pub robust: bool,

    /// Robust estimation method ("mcd", "robust_cov")
    pub robust_method: String,

    /// Contamination fraction for robust estimation
    /// Proportion of outliers expected in the data
    pub contamination: Float,

    /// Maximum number of iterations for robust estimation
    pub max_iter: usize,

    /// Whether to shrink covariance matrices toward diagonal
    pub shrinkage: Option<Float>,

    /// Method for handling singular covariance matrices
    pub singular_handling: String,
}

impl Default for QuadraticDiscriminantAnalysisConfig {
    fn default() -> Self {
        Self {
            priors: None,
            reg_param: 0.0,
            store_covariance: true,
            tol: 1e-4,
            diagonal_covariance: false,
            robust: false,
            robust_method: "mcd".to_string(),
            contamination: 0.1,
            max_iter: 100,
            shrinkage: None,
            singular_handling: "regularize".to_string(),
        }
    }
}

/// Quadratic Discriminant Analysis
///
/// A classifier with a quadratic decision boundary, generated by fitting class
/// conditional densities to the data and using Bayes' rule. Each class has its
/// own covariance matrix, allowing for more flexible decision boundaries than LDA.
///
/// # Mathematical Background
///
/// QDA models each class as a Gaussian distribution with its own mean μᵢ and
/// covariance matrix Σᵢ. The discriminant function for class i is:
///
/// δᵢ(x) = -½(x-μᵢ)ᵀΣᵢ⁻¹(x-μᵢ) - ½log|Σᵢ| + log(πᵢ)
///
/// where πᵢ is the prior probability of class i.
///
/// # Examples
///
/// ```rust,ignore
/// use sklears_discriminant_analysis::QuadraticDiscriminantAnalysis;
/// use scirs2_core::ndarray::array;
///
/// let qda = QuadraticDiscriminantAnalysis::new()
///     .reg_param(0.01)
///     .diagonal_covariance(false);
///
/// let X = array![[1.0, 2.0], [2.0, 3.0], [3.0, 1.0], [1.0, 3.0]];
/// let y = array![0, 0, 1, 1];
///
/// let trained_qda = qda.fit(&X, &y).unwrap();
/// let predictions = trained_qda.predict(&X).unwrap();
/// let probabilities = trained_qda.predict_proba(&X).unwrap();
/// ```
#[derive(Debug, Clone)]
pub struct QuadraticDiscriminantAnalysis<State = Untrained> {
    config: QuadraticDiscriminantAnalysisConfig,
    state: PhantomData<State>,
    // Trained state fields
    classes_: Option<Array1<i32>>,
    means_: Option<Array2<Float>>,
    covariances_: Option<Vec<Array2<Float>>>,
    covariance_determinants_: Option<Array1<Float>>,
    covariance_inverses_: Option<Vec<Array2<Float>>>,
    priors_: Option<Array1<Float>>,
    n_features_: Option<usize>,
    robust_weights_: Option<Vec<Array1<Float>>>,
}

impl QuadraticDiscriminantAnalysis<Untrained> {
    /// Create a new QuadraticDiscriminantAnalysis instance
    pub fn new() -> Self {
        Self {
            config: QuadraticDiscriminantAnalysisConfig::default(),
            state: PhantomData,
            classes_: None,
            means_: None,
            covariances_: None,
            covariance_determinants_: None,
            covariance_inverses_: None,
            priors_: None,
            n_features_: None,
            robust_weights_: None,
        }
    }

    /// Set the prior probabilities
    pub fn priors(mut self, priors: Option<Array1<Float>>) -> Self {
        self.config.priors = priors;
        self
    }

    /// Set the regularization parameter
    pub fn reg_param(mut self, reg_param: Float) -> Self {
        self.config.reg_param = reg_param;
        self
    }

    /// Set whether to store covariance matrices
    pub fn store_covariance(mut self, store_covariance: bool) -> Self {
        self.config.store_covariance = store_covariance;
        self
    }

    /// Set the tolerance
    pub fn tol(mut self, tol: Float) -> Self {
        self.config.tol = tol;
        self
    }

    /// Set whether to use diagonal covariance matrices
    pub fn diagonal_covariance(mut self, diagonal_covariance: bool) -> Self {
        self.config.diagonal_covariance = diagonal_covariance;
        self
    }

    /// Set whether to use robust estimation
    pub fn robust(mut self, robust: bool) -> Self {
        self.config.robust = robust;
        self
    }

    /// Set the robust estimation method
    pub fn robust_method(mut self, robust_method: &str) -> Self {
        self.config.robust_method = robust_method.to_string();
        self
    }

    /// Set the contamination fraction for robust estimation
    pub fn contamination(mut self, contamination: Float) -> Self {
        self.config.contamination = contamination;
        self
    }

    /// Set the maximum number of iterations
    pub fn max_iter(mut self, max_iter: usize) -> Self {
        self.config.max_iter = max_iter;
        self
    }

    /// Set the shrinkage parameter for covariance estimation
    pub fn shrinkage(mut self, shrinkage: Option<Float>) -> Self {
        self.config.shrinkage = shrinkage;
        self
    }

    /// Set the method for handling singular covariance matrices
    pub fn singular_handling(mut self, method: &str) -> Self {
        self.config.singular_handling = method.to_string();
        self
    }

    /// Compute robust mean using iterative reweighting
    fn compute_robust_mean(&self, data: &Array2<Float>, weights: &Array1<Float>) -> Array1<Float> {
        let (n_samples, n_features) = data.dim();
        let mut robust_mean = Array1::zeros(n_features);

        let weight_sum: Float = weights.sum();
        if weight_sum > 0.0 {
            for j in 0..n_features {
                let weighted_sum: Float = data
                    .column(j)
                    .iter()
                    .zip(weights.iter())
                    .map(|(&x, &w)| x * w)
                    .sum();
                robust_mean[j] = weighted_sum / weight_sum;
            }
        }

        robust_mean
    }

    /// Compute robust covariance using Minimum Covariance Determinant (MCD)
    fn compute_robust_covariance_mcd(
        &self,
        data: &Array2<Float>,
    ) -> SklResult<(Array1<Float>, Array2<Float>, Array1<Float>)> {
        let (n_samples, n_features) = data.dim();
        let subset_size = ((n_samples as Float * (1.0 - self.config.contamination)).ceil() as usize)
            .max(n_features + 1)
            .min(n_samples);

        let mut best_mean = data.mean_axis(Axis(0)).unwrap();
        let mut best_cov = Array2::eye(n_features);
        let mut best_det = Float::INFINITY;
        let mut best_weights = Array1::ones(n_samples);

        // Try multiple random subsets for robust estimation
        for trial in 0..self.config.max_iter.min(50) {
            let mut indices: Vec<usize> = (0..n_samples).collect();

            // Simple pseudo-random shuffle using trial number for reproducibility
            for i in 0..n_samples {
                let j = (i + trial * 17 + 31) % n_samples;
                indices.swap(i, j);
            }
            indices.truncate(subset_size);

            // Compute subset mean and covariance
            let subset_data: Array2<Float> =
                Array2::from_shape_fn((subset_size, n_features), |(i, j)| data[[indices[i], j]]);

            let subset_mean = subset_data.mean_axis(Axis(0)).unwrap();
            let mut subset_cov = Array2::zeros((n_features, n_features));

            // Compute covariance for subset
            for i in 0..subset_size {
                let row = subset_data.row(i);
                let diff = &row - &subset_mean;
                for j in 0..n_features {
                    for k in 0..n_features {
                        subset_cov[[j, k]] += diff[j] * diff[k];
                    }
                }
            }

            if subset_size > 1 {
                subset_cov = subset_cov / (subset_size - 1) as Float;
            }

            // Add regularization for numerical stability
            for i in 0..n_features {
                subset_cov[[i, i]] += self.config.tol;
            }

            // Compute determinant (simplified approach)
            let det: Float = subset_cov.diag().iter().product();

            if det > 0.0 && det < best_det {
                best_det = det;
                best_mean = subset_mean;
                best_cov = subset_cov;

                // Compute Mahalanobis distances and weights
                if let Ok(cov_inv) = self.compute_matrix_inverse(&best_cov) {
                    for i in 0..n_samples {
                        let sample = data.row(i);
                        let diff = &sample - &best_mean;
                        let mahal_dist_sq = self.quadratic_form(&diff, &cov_inv);

                        // Robust weight using Tukey's biweight function
                        let c = 4.685; // Tuning constant for 95% efficiency
                        let mahal_dist = mahal_dist_sq.sqrt();
                        if mahal_dist <= c {
                            let u = mahal_dist / c;
                            best_weights[i] = (1.0 - u * u).powi(2);
                        } else {
                            best_weights[i] = 0.0;
                        }
                    }
                }
            }
        }

        Ok((best_mean, best_cov, best_weights))
    }

    /// Compute matrix inverse using LU decomposition with partial pivoting
    fn compute_matrix_inverse(&self, matrix: &Array2<Float>) -> SklResult<Array2<Float>> {
        let n = matrix.nrows();
        let mut a = matrix.clone();
        let mut b = Array2::eye(n);

        // Forward elimination with partial pivoting
        for i in 0..n {
            // Find pivot
            let mut max_row = i;
            for k in (i + 1)..n {
                if a[[k, i]].abs() > a[[max_row, i]].abs() {
                    max_row = k;
                }
            }

            // Swap rows if needed
            if max_row != i {
                for j in 0..n {
                    a.swap([i, j], [max_row, j]);
                    b.swap([i, j], [max_row, j]);
                }
            }

            // Check for singularity and handle it
            if a[[i, i]].abs() < self.config.tol {
                match self.config.singular_handling.as_str() {
                    "regularize" => {
                        a[[i, i]] += self.config.reg_param + self.config.tol;
                    },
                    "pseudoinverse" => {
                        // Simplified pseudoinverse approach
                        a[[i, i]] = self.config.tol;
                    },
                    _ => {
                        return Err(SklearsError::NumericalError(
                            "Singular covariance matrix encountered".to_string()
                        ));
                    }
                }
            }

            // Eliminate column
            for k in (i + 1)..n {
                let factor = a[[k, i]] / a[[i, i]];
                for j in i..n {
                    a[[k, j]] -= factor * a[[i, j]];
                }
                for j in 0..n {
                    b[[k, j]] -= factor * b[[i, j]];
                }
            }
        }

        // Back substitution
        for i in (0..n).rev() {
            for j in 0..n {
                for k in (i + 1)..n {
                    b[[i, j]] -= a[[i, k]] * b[[k, j]];
                }
                b[[i, j]] /= a[[i, i]];
            }
        }

        Ok(b)
    }

    /// Compute quadratic form x^T A x efficiently
    fn quadratic_form(&self, x: &ArrayView1<Float>, a: &Array2<Float>) -> Float {
        let n = x.len();
        let mut result = 0.0;

        for i in 0..n {
            for j in 0..n {
                result += x[i] * a[[i, j]] * x[j];
            }
        }

        result
    }

    /// Apply shrinkage to covariance matrix
    fn apply_shrinkage(&self, cov: &Array2<Float>, shrinkage: Float) -> Array2<Float> {
        let n = cov.nrows();
        let trace = cov.diag().sum();
        let identity_scaled = Array2::eye(n) * (trace / n as Float);

        &cov * (1.0 - shrinkage) + &identity_scaled * shrinkage
    }

    /// Compute log determinant safely
    fn safe_log_determinant(&self, matrix: &Array2<Float>) -> Float {
        if self.config.diagonal_covariance {
            // For diagonal matrices, determinant is product of diagonal elements
            matrix.diag().iter().map(|&x| (x + self.config.tol).ln()).sum()
        } else {
            // Simplified approach using product of diagonal elements
            // In practice, you'd use proper LU decomposition or Cholesky
            let diag_product: Float = matrix.diag().iter()
                .map(|&x| (x + self.config.tol))
                .product();
            diag_product.ln()
        }
    }
}

impl Default for QuadraticDiscriminantAnalysis<Untrained> {
    fn default() -> Self {
        Self::new()
    }
}

impl Estimator for QuadraticDiscriminantAnalysis<Untrained> {
    type Config = QuadraticDiscriminantAnalysisConfig;
    type Error = SklearsError;
    type Float = Float;

    fn config(&self) -> &Self::Config {
        &self.config
    }
}

impl Fit<Array2<Float>, Array1<i32>> for QuadraticDiscriminantAnalysis<Untrained> {
    type Fitted = QuadraticDiscriminantAnalysis<Trained>;

    fn fit(self, x: &Array2<Float>, y: &Array1<i32>) -> SklResult<Self::Fitted> {
        // Basic validation
        validate::check_consistent_length(x, y)?;

        let (n_samples, n_features) = x.dim();
        if n_samples == 0 {
            return Err(SklearsError::InvalidInput(
                "No samples provided".to_string(),
            ));
        }

        if n_features == 0 {
            return Err(SklearsError::InvalidInput(
                "No features provided".to_string(),
            ));
        }

        // Get unique classes
        let mut classes: Vec<i32> = y
            .iter()
            .cloned()
            .collect::<std::collections::HashSet<_>>()
            .into_iter()
            .collect();
        classes.sort();
        let n_classes = classes.len();

        if n_classes < 2 {
            return Err(SklearsError::InvalidInput(
                "Need at least 2 classes".to_string(),
            ));
        }

        // Calculate class means, covariances, and related quantities
        let mut means = Array2::zeros((n_classes, n_features));
        let mut covariances = Vec::with_capacity(n_classes);
        let mut covariance_inverses = Vec::with_capacity(n_classes);
        let mut covariance_determinants = Array1::zeros(n_classes);
        let mut class_counts = Array1::zeros(n_classes);
        let mut robust_weights = Vec::new();

        for (i, &class) in classes.iter().enumerate() {
            let class_mask: Vec<bool> = y.iter().map(|&yi| yi == class).collect();
            let class_samples: Vec<ArrayView1<Float>> = x
                .axis_iter(Axis(0))
                .enumerate()
                .filter(|(j, _)| class_mask[*j])
                .map(|(_, sample)| sample)
                .collect();

            let count = class_samples.len();
            class_counts[i] = count as Float;

            if count == 0 {
                return Err(SklearsError::InvalidInput(format!(
                    "Class {} has no samples",
                    class
                )));
            }

            if count <= n_features && !self.config.diagonal_covariance {
                return Err(SklearsError::InvalidInput(format!(
                    "Class {} has {} samples but {} features. Use diagonal_covariance=true or provide more samples",
                    class, count, n_features
                )));
            }

            // Calculate class mean and covariance (robust or standard)
            let (class_mean, mut cov, weights) = if self.config.robust && self.config.robust_method == "mcd" {
                // Convert class samples to matrix for robust estimation
                let class_data: Array2<Float> = Array2::from_shape_fn((count, n_features), |(i, j)| {
                    class_samples[i][j]
                });

                // Use robust estimation
                match self.compute_robust_covariance_mcd(&class_data) {
                    Ok((robust_mean, robust_cov, weights)) => (robust_mean, robust_cov, Some(weights)),
                    Err(_) => {
                        // Fall back to standard estimation if robust fails
                        self.standard_estimation(&class_samples, n_features, count)?
                    }
                }
            } else {
                // Standard estimation
                self.standard_estimation(&class_samples, n_features, count)?
            };

            means.row_mut(i).assign(&class_mean);

            // Apply shrinkage if specified
            if let Some(shrinkage) = self.config.shrinkage {
                cov = self.apply_shrinkage(&cov, shrinkage);
            }

            // Add regularization
            if self.config.reg_param > 0.0 {
                for j in 0..n_features {
                    cov[[j, j]] += self.config.reg_param;
                }
            }

            // Compute determinant and inverse
            let log_det = self.safe_log_determinant(&cov);
            covariance_determinants[i] = log_det;

            let cov_inv = self.compute_matrix_inverse(&cov)?;

            covariances.push(cov);
            covariance_inverses.push(cov_inv);

            if let Some(w) = weights {
                robust_weights.push(w);
            }
        }

        // Calculate priors
        let priors = if let Some(ref p) = self.config.priors {
            if p.len() != n_classes {
                return Err(SklearsError::InvalidInput(
                    "Number of priors must match number of classes".to_string(),
                ));
            }
            p.clone()
        } else {
            &class_counts / n_samples as Float
        };

        Ok(QuadraticDiscriminantAnalysis {
            config: self.config,
            state: PhantomData,
            classes_: Some(Array1::from(classes)),
            means_: Some(means),
            covariances_: Some(covariances),
            covariance_determinants_: Some(covariance_determinants),
            covariance_inverses_: Some(covariance_inverses),
            priors_: Some(priors),
            n_features_: Some(n_features),
            robust_weights_: if robust_weights.is_empty() {
                None
            } else {
                Some(robust_weights)
            },
        })
    }
}

impl QuadraticDiscriminantAnalysis<Untrained> {
    /// Standard (non-robust) estimation of mean and covariance
    fn standard_estimation(
        &self,
        class_samples: &[ArrayView1<Float>],
        n_features: usize,
        count: usize,
    ) -> SklResult<(Array1<Float>, Array2<Float>, Option<Array1<Float>>)> {
        // Compute class mean
        let mut class_mean = Array1::zeros(n_features);
        for sample in class_samples {
            for j in 0..n_features {
                class_mean[j] += sample[j];
            }
        }
        class_mean = class_mean / count as Float;

        // Compute covariance matrix
        let mut cov = Array2::zeros((n_features, n_features));
        if self.config.diagonal_covariance {
            // Diagonal covariance: only compute diagonal elements
            for sample in class_samples {
                let diff = sample - &class_mean;
                for j in 0..n_features {
                    cov[[j, j]] += diff[j] * diff[j];
                }
            }
        } else {
            // Full covariance matrix
            for sample in class_samples {
                let diff = sample - &class_mean;
                for j in 0..n_features {
                    for k in 0..n_features {
                        cov[[j, k]] += diff[j] * diff[k];
                    }
                }
            }
        }

        if count > 1 {
            cov = cov / (count - 1) as Float;
        }

        Ok((class_mean, cov, None))
    }
}

impl QuadraticDiscriminantAnalysis<Trained> {
    /// Get the classes
    pub fn classes(&self) -> &Array1<i32> {
        self.classes_.as_ref().expect("Model is trained")
    }

    /// Get the means
    pub fn means(&self) -> &Array2<Float> {
        self.means_.as_ref().expect("Model is trained")
    }

    /// Get the priors
    pub fn priors(&self) -> &Array1<Float> {
        self.priors_.as_ref().expect("Model is trained")
    }

    /// Get the covariance matrices
    pub fn covariances(&self) -> &Vec<Array2<Float>> {
        self.covariances_.as_ref().expect("Model is trained")
    }

    /// Get the covariance determinants
    pub fn covariance_determinants(&self) -> &Array1<Float> {
        self.covariance_determinants_.as_ref().expect("Model is trained")
    }

    /// Get the number of features
    pub fn n_features(&self) -> usize {
        self.n_features_.expect("Model is trained")
    }
}

impl Predict<Array2<Float>, Array1<i32>> for QuadraticDiscriminantAnalysis<Trained> {
    fn predict(&self, x: &Array2<Float>) -> SklResult<Array1<i32>> {
        let probas = self.predict_proba(x)?;
        let classes = self.classes_.as_ref().expect("Model is trained");

        let predictions: Vec<i32> = probas
            .axis_iter(Axis(0))
            .map(|row| {
                let max_idx = row
                    .iter()
                    .enumerate()
                    .max_by(|a, b| a.1.partial_cmp(b.1).unwrap_or(std::cmp::Ordering::Equal))
                    .unwrap()
                    .0;
                classes[max_idx]
            })
            .collect();

        Ok(Array1::from(predictions))
    }
}

impl PredictProba<Array2<Float>, Array2<Float>> for QuadraticDiscriminantAnalysis<Trained> {
    fn predict_proba(&self, x: &Array2<Float>) -> SklResult<Array2<Float>> {
        let n_features = self.n_features_.expect("Model is trained");
        validate::check_n_features(x, n_features)?;

        let means = self.means_.as_ref().expect("Model is trained");
        let covariances = self.covariances_.as_ref().expect("Model is trained");
        let covariance_inverses = self.covariance_inverses_.as_ref().expect("Model is trained");
        let covariance_determinants = self.covariance_determinants_.as_ref().expect("Model is trained");
        let priors = self.priors_.as_ref().expect("Model is trained");
        let n_classes = means.nrows();
        let n_samples = x.nrows();

        let mut log_likelihoods = Array2::zeros((n_samples, n_classes));

        for i in 0..n_classes {
            let class_mean = means.row(i);
            let cov_inv = &covariance_inverses[i];
            let log_det = covariance_determinants[i];

            for j in 0..n_samples {
                let sample = x.row(j);
                let diff = &sample - &class_mean;

                // Quadratic form calculation using precomputed inverse
                let quad_form = if self.config.diagonal_covariance {
                    // For diagonal covariance: (x-μ)ᵀ Σ⁻¹ (x-μ) = Σᵢ (xᵢ-μᵢ)²/σᵢ²
                    let mut sum = 0.0;
                    for k in 0..n_features {
                        sum += diff[k] * diff[k] * cov_inv[[k, k]];
                    }
                    sum
                } else {
                    // Full quadratic form using matrix multiplication
                    self.quadratic_form(&diff, cov_inv)
                };

                // Log-likelihood using multivariate Gaussian formula
                // log P(x|class_i) = log π_i - ½ log|Σ_i| - ½ (x-μ_i)ᵀ Σ_i⁻¹ (x-μ_i)
                log_likelihoods[[j, i]] = priors[i].ln() - 0.5 * (log_det + quad_form);
            }
        }

        // Convert log-likelihoods to probabilities using stable softmax
        let mut probabilities = Array2::zeros((n_samples, n_classes));
        for i in 0..n_samples {
            let row = log_likelihoods.row(i);
            let max_log_like = row.iter().fold(Float::NEG_INFINITY, |a, &b| a.max(b));

            let exp_likes: Vec<Float> = row.iter().map(|&x| (x - max_log_like).exp()).collect();
            let sum_exp: Float = exp_likes.iter().sum();

            if sum_exp > 0.0 {
                for j in 0..n_classes {
                    probabilities[[i, j]] = exp_likes[j] / sum_exp;
                }
            } else {
                // Fallback to uniform distribution if numerical issues
                for j in 0..n_classes {
                    probabilities[[i, j]] = 1.0 / n_classes as Float;
                }
            }
        }

        Ok(probabilities)
    }
}

impl QuadraticDiscriminantAnalysis<Trained> {
    /// Quadratic form computation for trained model
    fn quadratic_form(&self, x: &ArrayView1<Float>, a: &Array2<Float>) -> Float {
        let n = x.len();
        let mut result = 0.0;

        for i in 0..n {
            for j in 0..n {
                result += x[i] * a[[i, j]] * x[j];
            }
        }

        result
    }
}

#[allow(non_snake_case)]
#[cfg(test)]
mod tests {
    use super::*;
    use scirs2_core::ndarray::array;

    #[test]
    fn test_qda_basic() {
        let qda = QuadraticDiscriminantAnalysis::new();

        let X = array![
            [1.0, 2.0],
            [2.0, 3.0],
            [3.0, 1.0],
            [1.0, 3.0],
            [5.0, 6.0],
            [6.0, 7.0],
            [7.0, 5.0],
            [5.0, 7.0]
        ];
        let y = array![0, 0, 0, 0, 1, 1, 1, 1];

        let trained_qda = qda.fit(&X, &y).unwrap();
        let predictions = trained_qda.predict(&X).unwrap();
        let probabilities = trained_qda.predict_proba(&X).unwrap();

        assert_eq!(predictions.len(), 8);
        assert_eq!(probabilities.dim(), (8, 2));

        // Check that probabilities sum to 1
        for i in 0..8 {
            let sum: Float = probabilities.row(i).iter().sum();
            assert!((sum - 1.0).abs() < 1e-10);
        }
    }

    #[test]
    fn test_qda_with_regularization() {
        let qda = QuadraticDiscriminantAnalysis::new()
            .reg_param(0.1);

        let X = array![
            [1.0, 2.0],
            [2.0, 3.0],
            [5.0, 6.0],
            [6.0, 7.0]
        ];
        let y = array![0, 0, 1, 1];

        let result = qda.fit(&X, &y);
        assert!(result.is_ok());
    }

    #[test]
    fn test_qda_diagonal_covariance() {
        let qda = QuadraticDiscriminantAnalysis::new()
            .diagonal_covariance(true);

        let X = array![
            [1.0, 2.0, 3.0],
            [2.0, 3.0, 4.0],
            [5.0, 6.0, 7.0],
            [6.0, 7.0, 8.0]
        ];
        let y = array![0, 0, 1, 1];

        let result = qda.fit(&X, &y);
        assert!(result.is_ok());
    }

    #[test]
    fn test_qda_validation() {
        let qda = QuadraticDiscriminantAnalysis::new();

        let X = array![[1.0, 2.0], [3.0, 4.0]];
        let y = array![0]; // Wrong size

        let result = qda.fit(&X, &y);
        assert!(result.is_err());
    }

    #[test]
    fn test_qda_insufficient_samples() {
        let qda = QuadraticDiscriminantAnalysis::new();

        let X = array![[1.0, 2.0, 3.0]]; // 1 sample, 3 features
        let y = array![0];

        let result = qda.fit(&X, &y);
        assert!(result.is_err());
    }

    #[test]
    fn test_qda_robust_estimation() {
        let qda = QuadraticDiscriminantAnalysis::new()
            .robust(true)
            .contamination(0.2);

        let X = array![
            [1.0, 2.0],
            [2.0, 3.0],
            [3.0, 1.0],
            [1.0, 3.0],
            [5.0, 6.0],
            [6.0, 7.0],
            [7.0, 5.0],
            [5.0, 7.0],
            [100.0, 100.0], // Outlier
            [101.0, 101.0]  // Outlier
        ];
        let y = array![0, 0, 0, 0, 1, 1, 1, 1, 0, 1];

        let result = qda.fit(&X, &y);
        assert!(result.is_ok());
    }

    #[test]
    fn test_qda_getters() {
        let qda = QuadraticDiscriminantAnalysis::new();

        let X = array![
            [1.0, 2.0],
            [2.0, 3.0],
            [5.0, 6.0],
            [6.0, 7.0]
        ];
        let y = array![0, 0, 1, 1];

        let trained_qda = qda.fit(&X, &y).unwrap();

        assert_eq!(trained_qda.classes().len(), 2);
        assert_eq!(trained_qda.means().dim(), (2, 2));
        assert_eq!(trained_qda.priors().len(), 2);
        assert_eq!(trained_qda.covariances().len(), 2);
        assert_eq!(trained_qda.n_features(), 2);
    }
}