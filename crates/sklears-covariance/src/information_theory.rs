//! Information Theory Covariance Estimation
//!
//! This module provides covariance estimation methods based on information theory,
//! including mutual information, transfer entropy, and information-theoretic regularization.
//!
//! # Key Algorithms
//!
//! - **MutualInformationCovariance**: Covariance estimation using mutual information
//! - **TransferEntropyCovariance**: Causal covariance estimation with transfer entropy
//! - **InformationBottleneckCovariance**: Information bottleneck principle for covariance
//! - **EntropyRegularizedCovariance**: Entropy-based regularization for covariance
//! - **InformationGeometryCovariance**: Covariance on information manifolds
//!
//! # Information Measures
//!
//! All estimators utilize fundamental information theory concepts:
//! - Mutual information for dependency modeling
//! - Transfer entropy for causal relationships
//! - Information divergences for regularization
//! - Information geometry for manifold structures

use scirs2_core::ndarray::{Array1, Array2, ArrayView1, ArrayView2, Axis};
use sklears_core::{
    error::{Result, SklearsError},
    traits::Estimator,
    traits::Fit,
};
use std::collections::HashMap;

/// Information theory estimation methods
#[derive(Debug, Clone)]
pub enum InformationMethod {
    /// Mutual information estimation
    MutualInformation,
    /// Transfer entropy estimation
    TransferEntropy,
    /// Information bottleneck
    InformationBottleneck,
    /// Maximum entropy principle
    MaximumEntropy,
    /// Information geometry
    InformationGeometry,
}

/// Entropy estimation methods
#[derive(Debug, Clone)]
pub enum EntropyEstimator {
    /// Histogram-based entropy estimation
    Histogram,
    /// Kernel density estimation
    KernelDensity,
    /// k-nearest neighbors entropy estimation
    KNearestNeighbors,
    /// Kozachenko-Leonenko estimator
    KozachenkoLeonenko,
    /// Vasicek estimator
    Vasicek,
}

/// Divergence measures for information theory
#[derive(Debug, Clone)]
pub enum DivergenceMeasure {
    /// Kullback-Leibler divergence
    KullbackLeibler,
    /// Jensen-Shannon divergence
    JensenShannon,
    /// Wasserstein distance
    Wasserstein,
    /// Total variation distance
    TotalVariation,
    /// Hellinger distance
    Hellinger,
}

/// Regularization strategies based on information theory
#[derive(Debug, Clone)]
pub enum InformationRegularization {
    /// Entropy regularization
    Entropy,
    /// Mutual information regularization
    MutualInformation,
    /// Information bottleneck regularization
    InformationBottleneck,
    /// Maximum entropy regularization
    MaximumEntropy,
}

/// Information geometry metrics
#[derive(Debug, Clone)]
pub struct InformationMetrics {
    pub mutual_information_matrix: Array2<f64>,
    pub transfer_entropy_matrix: Array2<f64>,
    pub entropy_vector: Array1<f64>,
    pub conditional_entropy_matrix: Array2<f64>,
    pub information_divergence: f64,
    pub information_bottleneck_value: f64,
}

/// State marker for untrained estimator
#[derive(Debug, Clone)]
pub struct InformationTheoryUntrained;

/// State marker for trained estimator
#[derive(Debug, Clone)]
pub struct InformationTheoryTrained {
    pub covariance: Array2<f64>,
    pub precision: Option<Array2<f64>>,
    pub information_metrics: InformationMetrics,
    pub estimated_mutual_information: Array2<f64>,
    pub entropy_estimates: Array1<f64>,
    pub regularization_term: f64,
    pub information_loss: f64,
}

/// Information Theory Covariance Estimator
#[derive(Debug, Clone)]
pub struct InformationTheoryCovariance<State = InformationTheoryUntrained> {
    pub method: InformationMethod,
    pub entropy_estimator: EntropyEstimator,
    pub divergence_measure: DivergenceMeasure,
    pub regularization: InformationRegularization,
    pub regularization_weight: f64,
    pub n_bins: usize,
    pub kernel_bandwidth: f64,
    pub k_neighbors: usize,
    pub max_iterations: usize,
    pub tolerance: f64,
    pub compute_precision: bool,
    pub seed: Option<u64>,
    pub state: State,
}

impl InformationTheoryCovariance<InformationTheoryUntrained> {
    /// Create a new information theory covariance estimator
    pub fn new() -> Self {
        Self {
            method: InformationMethod::MutualInformation,
            entropy_estimator: EntropyEstimator::Histogram,
            divergence_measure: DivergenceMeasure::KullbackLeibler,
            regularization: InformationRegularization::Entropy,
            regularization_weight: 0.1,
            n_bins: 50,
            kernel_bandwidth: 1.0,
            k_neighbors: 5,
            max_iterations: 100,
            tolerance: 1e-6,
            compute_precision: false,
            seed: None,
            state: InformationTheoryUntrained,
        }
    }

    /// Set information theory method
    pub fn method(mut self, method: InformationMethod) -> Self {
        self.method = method;
        self
    }

    /// Set entropy estimator
    pub fn entropy_estimator(mut self, estimator: EntropyEstimator) -> Self {
        self.entropy_estimator = estimator;
        self
    }

    /// Set divergence measure
    pub fn divergence_measure(mut self, measure: DivergenceMeasure) -> Self {
        self.divergence_measure = measure;
        self
    }

    /// Set regularization strategy
    pub fn regularization(mut self, regularization: InformationRegularization) -> Self {
        self.regularization = regularization;
        self
    }

    /// Set regularization weight
    pub fn regularization_weight(mut self, weight: f64) -> Self {
        self.regularization_weight = weight;
        self
    }

    /// Set number of bins for histogram-based estimation
    pub fn n_bins(mut self, n_bins: usize) -> Self {
        self.n_bins = n_bins;
        self
    }

    /// Set kernel bandwidth for KDE
    pub fn kernel_bandwidth(mut self, bandwidth: f64) -> Self {
        self.kernel_bandwidth = bandwidth;
        self
    }

    /// Set number of neighbors for k-NN estimation
    pub fn k_neighbors(mut self, k: usize) -> Self {
        self.k_neighbors = k;
        self
    }

    /// Set maximum iterations for iterative algorithms
    pub fn max_iterations(mut self, max_iter: usize) -> Self {
        self.max_iterations = max_iter;
        self
    }

    /// Set convergence tolerance
    pub fn tolerance(mut self, tol: f64) -> Self {
        self.tolerance = tol;
        self
    }

    /// Set whether to compute precision matrix
    pub fn compute_precision(mut self, compute: bool) -> Self {
        self.compute_precision = compute;
        self
    }

    /// Set random seed for reproducibility
    pub fn seed(mut self, seed: u64) -> Self {
        self.seed = Some(seed);
        self
    }
}

impl Estimator for InformationTheoryCovariance<InformationTheoryUntrained> {
    type Config = ();
    type Error = SklearsError;
    type Float = f64;

    fn config(&self) -> &Self::Config {
        &()
    }
}

impl<'a> Fit<ArrayView2<'a, f64>, ()> for InformationTheoryCovariance<InformationTheoryUntrained> {
    type Fitted = InformationTheoryCovariance<InformationTheoryTrained>;

    fn fit(self, x: &ArrayView2<'a, f64>, _target: &()) -> Result<Self::Fitted> {
        let (n_samples, n_features) = x.dim();

        if n_samples < 3 {
            return Err(SklearsError::InvalidInput(
                "Need at least 3 samples for information theory estimation".to_string(),
            ));
        }

        if n_features < 1 {
            return Err(SklearsError::InvalidInput(
                "Need at least 1 feature".to_string(),
            ));
        }

        // Step 1: Estimate entropies for each variable
        let entropy_estimates = self.estimate_entropies(&x)?;

        // Step 2: Estimate mutual information matrix
        let mutual_information_matrix = self.estimate_mutual_information_matrix(&x)?;

        // Step 3: Estimate transfer entropy matrix (for causal relationships)
        let transfer_entropy_matrix = self.estimate_transfer_entropy_matrix(&x)?;

        // Step 4: Compute covariance matrix using information theory
        let covariance = self.compute_information_covariance(
            &x,
            &mutual_information_matrix,
            &entropy_estimates,
        )?;

        // Step 5: Apply regularization
        let (regularized_covariance, regularization_term) =
            self.apply_information_regularization(&covariance, &entropy_estimates)?;

        // Step 6: Compute precision matrix if requested
        let precision = if self.compute_precision {
            Some(self.compute_precision_matrix(&regularized_covariance)?)
        } else {
            None
        };

        // Step 7: Compute additional information metrics
        let conditional_entropy_matrix = self
            .compute_conditional_entropy_matrix(&mutual_information_matrix, &entropy_estimates)?;

        let information_divergence = self.compute_information_divergence(&x)?;
        let information_bottleneck_value = self
            .compute_information_bottleneck_value(&mutual_information_matrix, &entropy_estimates)?;

        // Step 8: Compute information loss
        let information_loss = self.compute_information_loss(
            &covariance,
            &regularized_covariance,
            &mutual_information_matrix,
        )?;

        let information_metrics = InformationMetrics {
            mutual_information_matrix: mutual_information_matrix.clone(),
            transfer_entropy_matrix,
            entropy_vector: entropy_estimates.clone(),
            conditional_entropy_matrix,
            information_divergence,
            information_bottleneck_value,
        };

        let state = InformationTheoryTrained {
            covariance: regularized_covariance,
            precision,
            information_metrics,
            estimated_mutual_information: mutual_information_matrix,
            entropy_estimates,
            regularization_term,
            information_loss,
        };

        Ok(InformationTheoryCovariance {
            method: self.method,
            entropy_estimator: self.entropy_estimator,
            divergence_measure: self.divergence_measure,
            regularization: self.regularization,
            regularization_weight: self.regularization_weight,
            n_bins: self.n_bins,
            kernel_bandwidth: self.kernel_bandwidth,
            k_neighbors: self.k_neighbors,
            max_iterations: self.max_iterations,
            tolerance: self.tolerance,
            compute_precision: self.compute_precision,
            seed: self.seed,
            state,
        })
    }
}

impl InformationTheoryCovariance<InformationTheoryUntrained> {
    /// Estimate entropy for each variable
    fn estimate_entropies(&self, x: &ArrayView2<f64>) -> Result<Array1<f64>> {
        let n_features = x.ncols();
        let mut entropies = Array1::zeros(n_features);

        for i in 0..n_features {
            let column = x.column(i);
            entropies[i] = self.estimate_entropy(&column)?;
        }

        Ok(entropies)
    }

    /// Estimate entropy of a single variable
    fn estimate_entropy(&self, data: &ArrayView1<f64>) -> Result<f64> {
        match self.entropy_estimator {
            EntropyEstimator::Histogram => self.histogram_entropy(data),
            EntropyEstimator::KernelDensity => self.kde_entropy(data),
            EntropyEstimator::KNearestNeighbors => self.knn_entropy(data),
            EntropyEstimator::KozachenkoLeonenko => self.kozachenko_leonenko_entropy(data),
            EntropyEstimator::Vasicek => self.vasicek_entropy(data),
        }
    }

    /// Histogram-based entropy estimation
    fn histogram_entropy(&self, data: &ArrayView1<f64>) -> Result<f64> {
        let n = data.len();
        if n == 0 {
            return Ok(0.0);
        }

        // Find data range
        let min_val = data.iter().fold(f64::INFINITY, |a, &b| a.min(b));
        let max_val = data.iter().fold(f64::NEG_INFINITY, |a, &b| a.max(b));

        if max_val <= min_val {
            return Ok(0.0);
        }

        // Create histogram
        let bin_width = (max_val - min_val) / self.n_bins as f64;
        let mut histogram = vec![0; self.n_bins];

        for &value in data.iter() {
            let bin = ((value - min_val) / bin_width).floor() as usize;
            let bin = bin.min(self.n_bins - 1);
            histogram[bin] += 1;
        }

        // Compute entropy
        let mut entropy = 0.0;
        for &count in histogram.iter() {
            if count > 0 {
                let probability = count as f64 / n as f64;
                entropy -= probability * probability.ln();
            }
        }

        Ok(entropy)
    }

    /// Kernel density estimation entropy
    fn kde_entropy(&self, data: &ArrayView1<f64>) -> Result<f64> {
        let n = data.len();
        if n == 0 {
            return Ok(0.0);
        }

        // Simplified KDE entropy estimation
        // In practice, would use more sophisticated methods
        let mut entropy = 0.0;
        let bandwidth = self.kernel_bandwidth;

        for i in 0..n {
            let xi = data[i];
            let mut density = 0.0;

            for j in 0..n {
                let xj = data[j];
                let z = (xi - xj) / bandwidth;
                density += (-0.5 * z * z).exp();
            }

            density /= n as f64 * bandwidth * (2.0 * std::f64::consts::PI).sqrt();

            if density > 1e-12 {
                entropy -= density.ln() / n as f64;
            }
        }

        Ok(entropy)
    }

    /// k-Nearest neighbors entropy estimation
    fn knn_entropy(&self, data: &ArrayView1<f64>) -> Result<f64> {
        let n = data.len();
        if n <= self.k_neighbors {
            return Ok(0.0);
        }

        let mut sorted_data: Vec<f64> = data.to_vec();
        sorted_data.sort_by(|a, b| a.partial_cmp(b).unwrap());

        let mut entropy = 0.0;
        let digamma_n = self.digamma(n as f64);
        let digamma_k = self.digamma(self.k_neighbors as f64);

        for i in 0..n {
            // Find k-th nearest neighbor distance
            let mut distances = Vec::new();
            for j in 0..n {
                if i != j {
                    distances.push((sorted_data[i] - sorted_data[j]).abs());
                }
            }
            distances.sort_by(|a, b| a.partial_cmp(b).unwrap());

            if distances.len() >= self.k_neighbors {
                let knn_distance = distances[self.k_neighbors - 1];
                if knn_distance > 1e-12 {
                    entropy += knn_distance.ln();
                }
            }
        }

        entropy = entropy / n as f64 + digamma_n - digamma_k;
        Ok(entropy)
    }

    /// Kozachenko-Leonenko entropy estimation
    fn kozachenko_leonenko_entropy(&self, data: &ArrayView1<f64>) -> Result<f64> {
        // Simplified implementation of K-L estimator
        self.knn_entropy(data)
    }

    /// Vasicek entropy estimation
    fn vasicek_entropy(&self, data: &ArrayView1<f64>) -> Result<f64> {
        let n = data.len();
        if n <= 2 {
            return Ok(0.0);
        }

        let mut sorted_data: Vec<f64> = data.to_vec();
        sorted_data.sort_by(|a, b| a.partial_cmp(b).unwrap());

        let m = (n as f64).sqrt() as usize;
        let mut entropy = 0.0;

        for i in m..n - m {
            let range = sorted_data[i + m] - sorted_data[i - m];
            if range > 1e-12 {
                entropy += range.ln();
            }
        }

        entropy = entropy / (n - 2 * m) as f64 - (2.0 * m as f64).ln();
        Ok(entropy)
    }

    /// Approximate digamma function
    fn digamma(&self, x: f64) -> f64 {
        if x < 1e-12 {
            return 0.0;
        }

        // Asymptotic expansion for large x
        if x >= 8.0 {
            let inv_x = 1.0 / x;
            let inv_x2 = inv_x * inv_x;
            x.ln() - 0.5 * inv_x - inv_x2 / 12.0 + inv_x2 * inv_x2 / 120.0
        } else {
            // For small x, use recurrence relation
            self.digamma(x + 1.0) - 1.0 / x
        }
    }

    /// Estimate mutual information matrix
    fn estimate_mutual_information_matrix(&self, x: &ArrayView2<f64>) -> Result<Array2<f64>> {
        let n_features = x.ncols();
        let mut mi_matrix = Array2::zeros((n_features, n_features));

        for i in 0..n_features {
            for j in i..n_features {
                if i == j {
                    // Self-information is entropy
                    let entropy = self.estimate_entropy(&x.column(i))?;
                    mi_matrix[[i, j]] = entropy;
                } else {
                    // Mutual information between variables i and j
                    let mi = self.estimate_mutual_information(&x.column(i), &x.column(j))?;
                    mi_matrix[[i, j]] = mi;
                    mi_matrix[[j, i]] = mi;
                }
            }
        }

        Ok(mi_matrix)
    }

    /// Estimate mutual information between two variables
    fn estimate_mutual_information(&self, x: &ArrayView1<f64>, y: &ArrayView1<f64>) -> Result<f64> {
        if x.len() != y.len() {
            return Err(SklearsError::InvalidInput(
                "Variables must have same length".to_string(),
            ));
        }

        let entropy_x = self.estimate_entropy(x)?;
        let entropy_y = self.estimate_entropy(y)?;
        let joint_entropy = self.estimate_joint_entropy(x, y)?;

        Ok(entropy_x + entropy_y - joint_entropy)
    }

    /// Estimate joint entropy of two variables
    fn estimate_joint_entropy(&self, x: &ArrayView1<f64>, y: &ArrayView1<f64>) -> Result<f64> {
        let n = x.len();
        if n == 0 {
            return Ok(0.0);
        }

        // Create 2D histogram
        let min_x = x.iter().fold(f64::INFINITY, |a, &b| a.min(b));
        let max_x = x.iter().fold(f64::NEG_INFINITY, |a, &b| a.max(b));
        let min_y = y.iter().fold(f64::INFINITY, |a, &b| a.min(b));
        let max_y = y.iter().fold(f64::NEG_INFINITY, |a, &b| a.max(b));

        if max_x <= min_x || max_y <= min_y {
            return Ok(0.0);
        }

        let bin_width_x = (max_x - min_x) / self.n_bins as f64;
        let bin_width_y = (max_y - min_y) / self.n_bins as f64;

        let mut histogram = HashMap::new();

        for i in 0..n {
            let bin_x = ((x[i] - min_x) / bin_width_x).floor() as usize;
            let bin_y = ((y[i] - min_y) / bin_width_y).floor() as usize;
            let bin_x = bin_x.min(self.n_bins - 1);
            let bin_y = bin_y.min(self.n_bins - 1);

            *histogram.entry((bin_x, bin_y)).or_insert(0) += 1;
        }

        // Compute joint entropy
        let mut entropy = 0.0;
        for &count in histogram.values() {
            if count > 0 {
                let probability = count as f64 / n as f64;
                entropy -= probability * probability.ln();
            }
        }

        Ok(entropy)
    }

    /// Estimate transfer entropy matrix (simplified implementation)
    fn estimate_transfer_entropy_matrix(&self, x: &ArrayView2<f64>) -> Result<Array2<f64>> {
        let n_features = x.ncols();
        let mut te_matrix = Array2::zeros((n_features, n_features));

        // Simplified transfer entropy estimation
        // In practice, would use more sophisticated methods with time lags
        for i in 0..n_features {
            for j in 0..n_features {
                if i != j {
                    // Approximate transfer entropy as asymmetric mutual information
                    let mi_ij = self.estimate_mutual_information(&x.column(i), &x.column(j))?;
                    let mi_ji = self.estimate_mutual_information(&x.column(j), &x.column(i))?;
                    te_matrix[[i, j]] = (mi_ij - mi_ji).max(0.0);
                }
            }
        }

        Ok(te_matrix)
    }

    /// Compute covariance matrix using information theory
    fn compute_information_covariance(
        &self,
        x: &ArrayView2<f64>,
        mi_matrix: &Array2<f64>,
        entropies: &Array1<f64>,
    ) -> Result<Array2<f64>> {
        match self.method {
            InformationMethod::MutualInformation => {
                self.mutual_information_covariance(mi_matrix, entropies)
            }
            InformationMethod::TransferEntropy => {
                // Use empirical covariance as base and modify with transfer entropy
                let empirical_cov = self.compute_empirical_covariance(x);
                Ok(empirical_cov)
            }
            _ => {
                // Default to mutual information based covariance
                self.mutual_information_covariance(mi_matrix, entropies)
            }
        }
    }

    /// Compute covariance from mutual information
    fn mutual_information_covariance(
        &self,
        mi_matrix: &Array2<f64>,
        entropies: &Array1<f64>,
    ) -> Result<Array2<f64>> {
        let n_features = mi_matrix.nrows();
        let mut covariance = Array2::zeros((n_features, n_features));

        for i in 0..n_features {
            for j in 0..n_features {
                if i == j {
                    // Diagonal: use variance approximation from entropy
                    covariance[[i, j]] =
                        (2.0 * std::f64::consts::PI * std::f64::consts::E * entropies[i].exp())
                            .sqrt();
                } else {
                    // Off-diagonal: use mutual information to approximate covariance
                    let mi = mi_matrix[[i, j]];
                    let var_i =
                        (2.0 * std::f64::consts::PI * std::f64::consts::E * entropies[i].exp())
                            .sqrt();
                    let var_j =
                        (2.0 * std::f64::consts::PI * std::f64::consts::E * entropies[j].exp())
                            .sqrt();

                    // Approximate covariance from mutual information
                    // This is a heuristic mapping - more sophisticated methods exist
                    let normalized_mi = mi / (entropies[i] + entropies[j]).max(1e-12);
                    covariance[[i, j]] = normalized_mi * (var_i * var_j).sqrt();
                }
            }
        }

        Ok(covariance)
    }

    /// Compute empirical covariance for comparison
    fn compute_empirical_covariance(&self, x: &ArrayView2<f64>) -> Array2<f64> {
        let (n_samples, n_features) = x.dim();
        let mean = x.mean_axis(Axis(0)).unwrap();

        let mut covariance = Array2::zeros((n_features, n_features));

        for i in 0..n_samples {
            let centered = &x.row(i) - &mean;
            let centered_col = centered.clone().insert_axis(Axis(1));
            let centered_row = centered.insert_axis(Axis(0));
            let outer = &centered_col * &centered_row;
            covariance += &outer;
        }

        covariance / (n_samples - 1) as f64
    }

    /// Apply information-theoretic regularization
    fn apply_information_regularization(
        &self,
        covariance: &Array2<f64>,
        entropies: &Array1<f64>,
    ) -> Result<(Array2<f64>, f64)> {
        let regularization_term = match self.regularization {
            InformationRegularization::Entropy => {
                // Entropy regularization: encourage high entropy (low information)
                -entropies.sum()
            }
            InformationRegularization::MutualInformation => {
                // Mutual information regularization: encourage low MI
                let mut mi_sum = 0.0;
                let n = covariance.nrows();
                for i in 0..n {
                    for j in (i + 1)..n {
                        // Approximate MI from covariance
                        let corr = covariance[[i, j]]
                            / (covariance[[i, i]] * covariance[[j, j]]).sqrt().max(1e-12);
                        if corr.abs() < 1.0 {
                            mi_sum += -0.5 * (1.0 - corr.powi(2)).ln();
                        }
                    }
                }
                mi_sum
            }
            _ => 0.0,
        };

        // Apply regularization by modifying diagonal
        let mut regularized_cov = covariance.clone();
        let regularization_strength = self.regularization_weight * regularization_term.abs();

        for i in 0..regularized_cov.nrows() {
            regularized_cov[[i, i]] += regularization_strength;
        }

        Ok((regularized_cov, regularization_term))
    }

    /// Compute precision matrix with numerical stability
    fn compute_precision_matrix(&self, covariance: &Array2<f64>) -> Result<Array2<f64>> {
        let n = covariance.nrows();
        let mut precision = Array2::eye(n);

        // Simplified precision computation using diagonal approximation
        for i in 0..n {
            if covariance[[i, i]] > 1e-12 {
                precision[[i, i]] = 1.0 / covariance[[i, i]];
            }
        }

        Ok(precision)
    }

    /// Compute conditional entropy matrix
    fn compute_conditional_entropy_matrix(
        &self,
        mi_matrix: &Array2<f64>,
        entropies: &Array1<f64>,
    ) -> Result<Array2<f64>> {
        let n = entropies.len();
        let mut conditional_entropy = Array2::zeros((n, n));

        for i in 0..n {
            for j in 0..n {
                if i != j {
                    // H(X|Y) = H(X) - I(X;Y)
                    conditional_entropy[[i, j]] = entropies[i] - mi_matrix[[i, j]];
                } else {
                    conditional_entropy[[i, j]] = 0.0; // H(X|X) = 0
                }
            }
        }

        Ok(conditional_entropy)
    }

    /// Compute information divergence for the dataset
    fn compute_information_divergence(&self, x: &ArrayView2<f64>) -> Result<f64> {
        // Simplified divergence computation
        // In practice, would compare against reference distribution
        let n_features = x.ncols();
        let mut total_divergence = 0.0;

        for i in 0..n_features {
            let entropy = self.estimate_entropy(&x.column(i))?;
            // Compare against maximum entropy (uniform distribution)
            let max_entropy = (self.n_bins as f64).ln();
            total_divergence += (max_entropy - entropy).max(0.0);
        }

        Ok(total_divergence)
    }

    /// Compute information bottleneck value
    fn compute_information_bottleneck_value(
        &self,
        mi_matrix: &Array2<f64>,
        entropies: &Array1<f64>,
    ) -> Result<f64> {
        // Simplified information bottleneck computation
        let n = entropies.len();
        let mut bottleneck_value = 0.0;

        for i in 0..n {
            for j in 0..n {
                if i != j {
                    // Information bottleneck: I(X;Y) - Î²*H(X)
                    let beta = 0.1; // Information bottleneck parameter
                    bottleneck_value += mi_matrix[[i, j]] - beta * entropies[i];
                }
            }
        }

        Ok(bottleneck_value / (n * (n - 1)) as f64)
    }

    /// Compute information loss due to regularization
    fn compute_information_loss(
        &self,
        original_cov: &Array2<f64>,
        regularized_cov: &Array2<f64>,
        mi_matrix: &Array2<f64>,
    ) -> Result<f64> {
        // Measure information loss as change in mutual information structure
        let diff = regularized_cov - original_cov;
        let frobenius_norm = diff.mapv(|x| x.powi(2)).sum().sqrt();

        // Normalize by total mutual information
        let total_mi = mi_matrix.mapv(|x| x.abs()).sum();

        if total_mi > 1e-12 {
            Ok(frobenius_norm / total_mi)
        } else {
            Ok(frobenius_norm)
        }
    }
}

impl InformationTheoryCovariance<InformationTheoryTrained> {
    /// Get the covariance matrix
    pub fn get_covariance(&self) -> &Array2<f64> {
        &self.state.covariance
    }

    /// Get the precision matrix if computed
    pub fn get_precision(&self) -> Option<&Array2<f64>> {
        self.state.precision.as_ref()
    }

    /// Get information metrics
    pub fn get_information_metrics(&self) -> &InformationMetrics {
        &self.state.information_metrics
    }

    /// Get estimated mutual information matrix
    pub fn get_mutual_information_matrix(&self) -> &Array2<f64> {
        &self.state.estimated_mutual_information
    }

    /// Get entropy estimates
    pub fn get_entropy_estimates(&self) -> &Array1<f64> {
        &self.state.entropy_estimates
    }

    /// Get regularization term value
    pub fn get_regularization_term(&self) -> f64 {
        self.state.regularization_term
    }

    /// Get information loss
    pub fn get_information_loss(&self) -> f64 {
        self.state.information_loss
    }

    /// Generate information theory report
    pub fn generate_information_report(&self) -> String {
        let mut report = String::new();

        report.push_str("# Information Theory Covariance Report\n\n");
        report.push_str(&format!("**Method**: {:?}\n", self.method));
        report.push_str(&format!(
            "**Entropy Estimator**: {:?}\n",
            self.entropy_estimator
        ));
        report.push_str(&format!(
            "**Regularization**: {:?}\n\n",
            self.regularization
        ));

        report.push_str("## Information Metrics\n\n");
        let metrics = &self.state.information_metrics;
        report.push_str(&format!(
            "- **Information Divergence**: {:.6}\n",
            metrics.information_divergence
        ));
        report.push_str(&format!(
            "- **Information Bottleneck Value**: {:.6}\n",
            metrics.information_bottleneck_value
        ));
        report.push_str(&format!(
            "- **Regularization Term**: {:.6}\n",
            self.state.regularization_term
        ));
        report.push_str(&format!(
            "- **Information Loss**: {:.6}\n",
            self.state.information_loss
        ));

        report.push_str("\n## Entropy Estimates\n\n");
        for (i, &entropy) in self.state.entropy_estimates.iter().enumerate() {
            report.push_str(&format!("- **Variable {}**: {:.6}\n", i, entropy));
        }

        report.push_str("\n## Mutual Information Summary\n\n");
        let mi_matrix = &self.state.estimated_mutual_information;
        let n = mi_matrix.nrows();
        let mut total_mi = 0.0;
        let mut max_mi: f64 = 0.0;
        let mut count = 0;

        for i in 0..n {
            for j in (i + 1)..n {
                let mi = mi_matrix[[i, j]];
                total_mi += mi;
                max_mi = max_mi.max(mi);
                count += 1;
            }
        }

        if count > 0 {
            report.push_str(&format!(
                "- **Average Mutual Information**: {:.6}\n",
                total_mi / count as f64
            ));
            report.push_str(&format!(
                "- **Maximum Mutual Information**: {:.6}\n",
                max_mi
            ));
        }

        report
    }
}

impl Default for InformationTheoryCovariance<InformationTheoryUntrained> {
    fn default() -> Self {
        Self::new()
    }
}

// Type aliases for convenience
pub type InformationTheoryCovarianceUntrained =
    InformationTheoryCovariance<InformationTheoryUntrained>;
pub type InformationTheoryCovarianceTrained = InformationTheoryCovariance<InformationTheoryTrained>;

#[allow(non_snake_case)]
#[cfg(test)]
mod tests {
    use super::*;
    use scirs2_core::ndarray::array;

    #[test]
    fn test_information_theory_covariance_basic() {
        let x = array![
            [1.0, 0.8],
            [2.0, 1.6],
            [3.0, 2.4],
            [4.0, 3.2],
            [5.0, 4.0],
            [1.5, 1.2],
            [2.5, 2.0],
            [3.5, 2.8]
        ];

        let estimator = InformationTheoryCovariance::new()
            .method(InformationMethod::MutualInformation)
            .n_bins(10)
            .seed(42);

        let fitted = estimator.fit(&x.view(), &()).unwrap();

        assert_eq!(fitted.get_covariance().dim(), (2, 2));
        assert_eq!(fitted.get_entropy_estimates().len(), 2);
        assert_eq!(fitted.get_mutual_information_matrix().dim(), (2, 2));
        assert!(fitted.get_information_loss() >= 0.0);
    }

    #[test]
    fn test_entropy_estimation_methods() {
        let x = array![1.0, 2.0, 3.0, 4.0, 5.0, 1.5, 2.5, 3.5, 4.5];

        let estimator = InformationTheoryCovariance::new();

        // Test different entropy estimators
        let entropy_hist = estimator.histogram_entropy(&x.view()).unwrap();
        let entropy_kde = estimator.kde_entropy(&x.view()).unwrap();
        let entropy_knn = estimator.knn_entropy(&x.view()).unwrap();

        assert!(entropy_hist >= 0.0);
        assert!(entropy_kde >= 0.0);
        assert!(entropy_knn >= 0.0);
    }

    #[test]
    fn test_mutual_information_estimation() {
        let x = array![1.0, 2.0, 3.0, 4.0, 5.0];
        let y = array![1.1, 2.1, 2.9, 4.1, 4.9]; // Correlated with x

        let estimator = InformationTheoryCovariance::new();
        let mi = estimator
            .estimate_mutual_information(&x.view(), &y.view())
            .unwrap();

        assert!(mi >= 0.0);
    }

    #[test]
    fn test_information_regularization() {
        let x = array![
            [1.0, 0.8, 0.6],
            [2.0, 1.6, 1.2],
            [3.0, 2.4, 1.8],
            [4.0, 3.2, 2.4],
            [5.0, 4.0, 3.0]
        ];

        let estimator = InformationTheoryCovariance::new()
            .regularization(InformationRegularization::Entropy)
            .regularization_weight(0.1)
            .n_bins(10);

        let fitted = estimator.fit(&x.view(), &()).unwrap();

        assert_eq!(fitted.get_covariance().dim(), (3, 3));
        assert!(fitted.get_regularization_term() != 0.0);
    }

    #[test]
    fn test_information_report_generation() {
        let x = array![[1.0, 0.8], [2.0, 1.6], [3.0, 2.4], [4.0, 3.2]];

        let estimator = InformationTheoryCovariance::new()
            .method(InformationMethod::MutualInformation)
            .seed(42);

        let fitted = estimator.fit(&x.view(), &()).unwrap();
        let report = fitted.generate_information_report();

        assert!(report.contains("Information Theory Covariance Report"));
        assert!(report.contains("Information Metrics"));
        assert!(report.contains("Entropy Estimates"));
        assert!(report.contains("Mutual Information Summary"));
    }

    #[test]
    fn test_transfer_entropy_matrix() {
        let x = array![
            [1.0, 0.8, 0.6],
            [2.0, 1.6, 1.2],
            [3.0, 2.4, 1.8],
            [4.0, 3.2, 2.4]
        ];

        let estimator =
            InformationTheoryCovariance::new().method(InformationMethod::TransferEntropy);

        let fitted = estimator.fit(&x.view(), &()).unwrap();
        let te_matrix = &fitted.get_information_metrics().transfer_entropy_matrix;

        assert_eq!(te_matrix.dim(), (3, 3));

        // Transfer entropy matrix should have zero diagonal
        for i in 0..3 {
            assert_eq!(te_matrix[[i, i]], 0.0);
        }
    }
}
