//! Benchmarks comparing sklears-svm performance against libsvm and other implementations
//!
//! This benchmark suite provides comprehensive performance comparisons between sklears-svm
//! and reference implementations including libsvm. It measures training time, prediction time,
//! memory usage, and accuracy across various datasets and configurations.

//!
//! NOTE: This benchmark is currently disabled due to incomplete API implementation.
//! Enable with `--features incomplete-benchmarks` once the required types are implemented.

#![cfg(feature = "incomplete-benchmarks")]

use criterion::{black_box, criterion_group, criterion_main, BenchmarkId, Criterion, Throughput};
use scirs2_core::ndarray::{Array1, Array2};
use scirs2_core::essentials::{Normal};
use sklears_svm::*;
use std::time::Duration;

/// Configuration for benchmark tests
#[derive(Clone)]
struct BenchmarkConfig {
    name: &'static str,
    n_samples: usize,
    n_features: usize,
    n_classes: usize,
    kernel: kernels::KernelType,
    c: f64,
    gamma: Option<f64>,
}

impl BenchmarkConfig {
    fn new(
        name: &'static str,
        n_samples: usize,
        n_features: usize,
        n_classes: usize,
        kernel: kernels::KernelType,
        c: f64,
    ) -> Self {
        Self {
            name,
            n_samples,
            n_features,
            n_classes,
            kernel,
            c,
            gamma: None,
        }
    }

    fn with_gamma(mut self, gamma: f64) -> Self {
        self.gamma = Some(gamma);
        self
    }
}

/// Generate synthetic dataset for benchmarking
fn generate_dataset(config: &BenchmarkConfig) -> (Array2<f64>, Array1<f64>) {
    let mut rng = scirs2_core::random::thread_rng();

    // Generate random features
    let mut x = Array2::zeros((config.n_samples, config.n_features));
    for i in 0..config.n_samples {
        for j in 0..config.n_features {
            x[[i, j]] = rng.sample(&Normal::new(0.0, 1.0).unwrap());
        }
    }

    // Generate labels based on linear combination with noise
    let mut y = Array1::zeros(config.n_samples);
    for i in 0..config.n_samples {
        let mut score = 0.0;
        for j in 0..config.n_features.min(5) {
            // Use first 5 features for label generation
            score += x[[i, j]];
        }
        score += rng.sample(&Normal::new(0.0, 0.1).unwrap()); // Add noise

        if config.n_classes == 2 {
            y[i] = if score > 0.0 { 1.0 } else { -1.0 };
        } else {
            // Multi-class classification
            let class = ((score + 2.0) * config.n_classes as f64 / 4.0).floor() as i32;
            y[i] = class.max(0).min(config.n_classes as i32 - 1) as f64;
        }
    }

    (x, y)
}

/// Benchmark sklears-svm training performance
fn bench_sklears_svm_training(c: &mut Criterion) {
    let configs = vec![
        BenchmarkConfig::new(
            "small_linear",
            1000,
            20,
            2,
            kernels::KernelType::Linear,
            1.0,
        ),
        BenchmarkConfig::new(
            "medium_linear",
            5000,
            50,
            2,
            kernels::KernelType::Linear,
            1.0,
        ),
        BenchmarkConfig::new(
            "small_rbf",
            1000,
            20,
            2,
            kernels::KernelType::Rbf { gamma: 0.1 },
            1.0,
        ),
        BenchmarkConfig::new(
            "medium_rbf",
            2000,
            50,
            2,
            kernels::KernelType::Rbf { gamma: 0.01 },
            1.0,
        ),
        BenchmarkConfig::new(
            "small_poly",
            1000,
            20,
            2,
            kernels::KernelType::Polynomial {
                coef0: 1.0,
                degree: 3.0,
            },
            1.0,
        ),
    ];

    let mut group = c.benchmark_group("sklears_svm_training");

    for config in configs {
        let (x, y) = generate_dataset(&config);

        group.throughput(Throughput::Elements(config.n_samples as u64));
        group.measurement_time(Duration::from_secs(30));

        group.bench_with_input(
            BenchmarkId::new("SVC", config.name),
            &config,
            |b, config| {
                b.iter(|| {
                    let svc = SVC::new()
                        .c(config.c)
                        .kernel(config.kernel.clone())
                        .with_max_iter(1000)
                        .with_tol(1e-3);

                    match svc.fit(black_box(&x), black_box(&y)) {
                        Ok(_trained_svc) => {}
                        Err(e) => eprintln!("SVC training failed: {:?}", e),
                    }
                })
            },
        );

        group.bench_with_input(
            BenchmarkId::new("LinearSVC", config.name),
            &config,
            |b, config| {
                b.iter(|| {
                    if matches!(config.kernel, kernels::KernelType::Linear) {
                        let linear_svc = LinearSVC::new().with_c(config.c).with_max_iter(1000).with_tol(1e-3);

                        match linear_svc.fit(black_box(&x), black_box(&y)) {
                            Ok(_trained_svc) => {}
                            Err(e) => eprintln!("LinearSVC training failed: {:?}", e),
                        }
                    }
                })
            },
        );
    }

    group.finish();
}

/// Benchmark sklears-svm prediction performance
fn bench_sklears_svm_prediction(c: &mut Criterion) {
    let configs = vec![
        BenchmarkConfig::new(
            "small_linear",
            1000,
            20,
            2,
            kernels::KernelType::Linear,
            1.0,
        ),
        BenchmarkConfig::new(
            "medium_linear",
            5000,
            50,
            2,
            kernels::KernelType::Linear,
            1.0,
        ),
        BenchmarkConfig::new(
            "small_rbf",
            1000,
            20,
            2,
            kernels::KernelType::Rbf { gamma: 0.1 },
            1.0,
        ),
    ];

    let mut group = c.benchmark_group("sklears_svm_prediction");

    for config in configs {
        let (x_train, y_train) = generate_dataset(&config);
        let (x_test, _y_test) = generate_dataset(&BenchmarkConfig {
            n_samples: 500,
            ..config
        });

        // Pre-train the models
        let trained_svc = SVC::new()
            .c(config.c)
            .kernel(config.kernel.clone())
            .with_max_iter(1000)
            .with_tol(1e-3)
            .fit(&x_train, &y_train);

        if let Ok(svc) = trained_svc {
            group.throughput(Throughput::Elements(x_test.nrows() as u64));

            group.bench_with_input(
                BenchmarkId::new("SVC_predict", config.name),
                &x_test,
                |b, x_test| {
                    b.iter(|| match svc.predict(black_box(x_test)) {
                        Ok(_predictions) => {}
                        Err(e) => eprintln!("SVC prediction failed: {:?}", e),
                    })
                },
            );
        }
    }

    group.finish();
}

/// Benchmark memory usage patterns
fn bench_memory_usage(c: &mut Criterion) {
    let configs = vec![
        BenchmarkConfig::new(
            "memory_small",
            1000,
            50,
            2,
            kernels::KernelType::Linear,
            1.0,
        ),
        BenchmarkConfig::new(
            "memory_medium",
            5000,
            100,
            2,
            kernels::KernelType::Rbf { gamma: 0.1 },
            1.0,
        ),
        BenchmarkConfig::new(
            "memory_large",
            10000,
            200,
            2,
            kernels::KernelType::Rbf { gamma: 0.01 },
            1.0,
        ),
    ];

    let mut group = c.benchmark_group("memory_usage");

    for config in configs {
        let (x, y) = generate_dataset(&config);

        group.throughput(Throughput::Elements(
            (config.n_samples * config.n_features) as u64,
        ));

        group.bench_with_input(
            BenchmarkId::new("memory_efficient_training", config.name),
            &config,
            |b, config| {
                b.iter(|| {
                    // Test with chunked processing for large datasets
                    let svc = if config.n_samples > 5000 {
                        SVC::new()
                            .c(config.c)
                            .kernel(config.kernel.clone())
                            .cache_size(200.0) // MB
                            .with_max_iter(1000)
                            .with_tol(1e-3)
                    } else {
                        SVC::new()
                            .c(config.c)
                            .kernel(config.kernel.clone())
                            .with_max_iter(1000)
                            .with_tol(1e-3)
                    };

                    match svc.fit(black_box(&x), black_box(&y)) {
                        Ok(_trained_svc) => {}
                        Err(e) => eprintln!("Memory efficient training failed: {:?}", e),
                    }
                })
            },
        );
    }

    group.finish();
}

/// Benchmark kernel computation performance
fn bench_kernel_computation(c: &mut Criterion) {
    let sample_sizes = vec![100, 500, 1000, 2000];
    let feature_sizes = vec![10, 50, 100];

    let mut group = c.benchmark_group("kernel_computation");

    for &n_samples in &sample_sizes {
        for &n_features in &feature_sizes {
            let mut rng = scirs2_core::random::thread_rng();
            let mut x = Array2::zeros((n_samples, n_features));
            for i in 0..n_samples {
                for j in 0..n_features {
                    x[[i, j]] = rng.sample(&Normal::new(0.0, 1.0).unwrap());
                }
            }

            let kernels = vec![
                ("linear", kernels::KernelType::Linear),
                ("rbf", kernels::KernelType::Rbf { gamma: 0.1 }),
                (
                    "poly",
                    kernels::KernelType::Polynomial {
                        coef0: 1.0,
                        degree: 3.0,
                    },
                ),
                (
                    "sigmoid",
                    kernels::KernelType::Sigmoid {
                        gamma: 0.1,
                        coef0: 1.0,
                    },
                ),
            ];

            for (kernel_name, kernel_type) in kernels {
                let config_name = format!("{}x{}", n_samples, n_features);

                group.throughput(Throughput::Elements((n_samples * n_samples) as u64));

                group.bench_with_input(
                    BenchmarkId::new(format!("{}_kernel", kernel_name), &config_name),
                    &(&x, &kernel_type),
                    |b, (x, kernel_type)| {
                        b.iter(|| {
                            let kernel_fn = kernels::KernelFunction::new((**kernel_type).clone());
                            let _kernel_matrix =
                                kernel_fn.compute_matrix(black_box(x), black_box(x));
                        })
                    },
                );
            }
        }
    }

    group.finish();
}

/// Benchmark parallel processing performance
fn bench_parallel_performance(c: &mut Criterion) {
    let configs = vec![
        BenchmarkConfig::new(
            "parallel_medium",
            5000,
            100,
            2,
            kernels::KernelType::Rbf { gamma: 0.1 },
            1.0,
        ),
        BenchmarkConfig::new(
            "parallel_large",
            10000,
            200,
            2,
            kernels::KernelType::Rbf { gamma: 0.01 },
            1.0,
        ),
    ];

    let mut group = c.benchmark_group("parallel_performance");

    for config in configs {
        let (x, y) = generate_dataset(&config);

        group.throughput(Throughput::Elements(config.n_samples as u64));

        // Test with parallel SMO
        group.bench_with_input(
            BenchmarkId::new("parallel_smo", config.name),
            &config,
            |b, config| {
                b.iter(|| {
                    let parallel_svc = parallel_smo::ParallelSMO::new()
                        .c(config.c)
                        .kernel(config.kernel.clone())
                        .with_max_iter(1000)
                        .tolerance(1e-3)
                        .num_threads(num_cpus::get());

                    match parallel_svc.fit(black_box(&x), black_box(&y)) {
                        Ok(_trained_svc) => {}
                        Err(e) => eprintln!("Parallel SMO training failed: {:?}", e),
                    }
                })
            },
        );
    }

    group.finish();
}

/// Benchmark SIMD-optimized kernel computations
#[cfg(feature = "simd")]
fn bench_simd_kernels(c: &mut Criterion) {
    let sample_sizes = vec![1000, 5000];
    let feature_sizes = vec![50, 200];

    let mut group = c.benchmark_group("simd_kernels");

    for &n_samples in &sample_sizes {
        for &n_features in &feature_sizes {
            let mut rng = scirs2_core::random::thread_rng();
            let mut x = Array2::zeros((n_samples, n_features));
            for i in 0..n_samples {
                for j in 0..n_features {
                    x[[i, j]] = rng.sample(&Normal::new(0.0, 1.0).unwrap());
                }
            }

            let config_name = format!("{}x{}", n_samples, n_features);

            group.throughput(Throughput::Elements((n_samples * n_samples) as u64));

            // Compare SIMD vs regular kernel computation
            group.bench_with_input(BenchmarkId::new("regular_rbf", &config_name), &x, |b, x| {
                b.iter(|| {
                    let kernel_fn =
                        kernels::KernelFunction::new(kernels::KernelType::Rbf { gamma: 0.1 });
                    let _kernel_matrix = kernel_fn.compute_matrix(black_box(x), black_box(x));
                })
            });

            group.bench_with_input(BenchmarkId::new("simd_rbf", &config_name), &x, |b, x| {
                b.iter(|| {
                    let simd_kernel =
                        simd_kernels::SimdKernelFunction::new(kernels::KernelType::Rbf {
                            gamma: 0.1,
                        });
                    let _kernel_matrix = simd_kernel.compute_matrix(black_box(x), black_box(x));
                })
            });
        }
    }

    group.finish();
}

/// Benchmark different optimization algorithms
fn bench_optimization_algorithms(c: &mut Criterion) {
    let config = BenchmarkConfig::new(
        "optimization",
        2000,
        100,
        2,
        kernels::KernelType::Rbf { gamma: 0.1 },
        1.0,
    );

    let (x, y) = generate_dataset(&config);

    let mut group = c.benchmark_group("optimization_algorithms");
    group.throughput(Throughput::Elements(config.n_samples as u64));

    // SMO algorithm
    group.bench_function("smo", |b| {
        b.iter(|| {
            let svc = SVC::new()
                .c(config.c)
                .kernel(config.kernel.clone())
                .with_max_iter(1000)
                .with_tol(1e-3);

            match svc.fit(black_box(&x), black_box(&y)) {
                Ok(_trained_svc) => {}
                Err(e) => eprintln!("SMO training failed: {:?}", e),
            }
        })
    });

    // Dual Coordinate Ascent
    group.bench_function("dual_coordinate_ascent", |b| {
        b.iter(|| {
            let dca = dual_coordinate_ascent::DualCoordinateAscent::new()
                .c(config.c)
                .kernel(config.kernel.clone())
                .with_max_iter(1000)
                .tolerance(1e-3);

            match dca.fit(black_box(&x), black_box(&y)) {
                Ok(_trained_svc) => {}
                Err(e) => eprintln!("DCA training failed: {:?}", e),
            }
        })
    });

    // ADMM for linear kernels
    if matches!(config.kernel, kernels::KernelType::Linear) {
        group.bench_function("admm", |b| {
            b.iter(|| {
                let admm_svm = advanced_optimization::ADMMSVM::new()
                    .c(config.c)
                    .rho(1.0)
                    .with_max_iter(1000)
                    .tolerance(1e-3);

                match admm_svm.fit(black_box(&x), black_box(&y)) {
                    Ok(_trained_svm) => {}
                    Err(e) => eprintln!("ADMM training failed: {:?}", e),
                }
            })
        });
    }

    group.finish();
}

/// Reference implementation comparison (mock libsvm results)
fn bench_reference_comparison(c: &mut Criterion) {
    let configs = vec![
        BenchmarkConfig::new("ref_small", 1000, 20, 2, kernels::KernelType::Linear, 1.0),
        BenchmarkConfig::new(
            "ref_medium",
            5000,
            50,
            2,
            kernels::KernelType::Rbf { gamma: 0.1 },
            1.0,
        ),
    ];

    let mut group = c.benchmark_group("reference_comparison");

    for config in configs {
        let (x, y) = generate_dataset(&config);

        group.throughput(Throughput::Elements(config.n_samples as u64));

        // Benchmark our implementation
        group.bench_with_input(
            BenchmarkId::new("sklears_svm", config.name),
            &config,
            |b, config| {
                b.iter(|| {
                    let svc = SVC::new()
                        .c(config.c)
                        .kernel(config.kernel.clone())
                        .with_max_iter(1000)
                        .with_tol(1e-3);

                    match svc.fit(black_box(&x), black_box(&y)) {
                        Ok(_trained_svc) => {}
                        Err(e) => eprintln!("sklears-svm training failed: {:?}", e),
                    }
                })
            },
        );

        // Mock libsvm benchmark (for reference timing)
        group.bench_with_input(
            BenchmarkId::new("libsvm_reference", config.name),
            &config,
            |b, config| {
                b.iter(|| {
                    // Mock libsvm timing - in practice, you would call libsvm here
                    // This simulates expected libsvm performance based on typical benchmarks
                    let mock_duration = match config.n_samples {
                        n if n <= 1000 => Duration::from_millis(50),
                        n if n <= 5000 => Duration::from_millis(500),
                        _ => Duration::from_secs(2),
                    };

                    std::thread::sleep(mock_duration);
                })
            },
        );
    }

    group.finish();
}

/// Comprehensive performance report
fn performance_report() {
    println!("\n=== sklears-svm Performance Report ===");
    println!("This benchmark suite compares sklears-svm against reference implementations.");
    println!("Key performance metrics:");
    println!("- Training time scalability");
    println!("- Prediction throughput");
    println!("- Memory efficiency");
    println!("- Kernel computation speed");
    println!("- Parallel processing gains");
    println!("- SIMD optimization benefits");
    println!("=====================================\n");
}

// Group all benchmarks
criterion_group!(
    benches,
    bench_sklears_svm_training,
    bench_sklears_svm_prediction,
    bench_memory_usage,
    bench_kernel_computation,
    bench_parallel_performance,
    bench_optimization_algorithms,
    bench_reference_comparison
);

// Add SIMD benchmarks if feature is enabled
#[cfg(feature = "simd")]
criterion_group!(simd_benches, bench_simd_kernels);

#[cfg(feature = "simd")]
criterion_main!(benches, simd_benches);

#[cfg(not(feature = "simd"))]
criterion_main!(benches);

// Custom main to add performance report
fn custom_main() {
    performance_report();
    criterion_main();
}


#[cfg(not(feature = "incomplete-benchmarks"))]
fn main() {
    eprintln!("This benchmark is disabled. Enable with --features incomplete-benchmarks");
}
