use sklears_core::traits::{Fit, Predict};
// Property-based tests for tree algorithms
//
// These tests use the proptest framework to verify that tree algorithms
// satisfy important mathematical and computational properties across
// a wide range of inputs.

use approx::assert_abs_diff_eq;
use proptest::prelude::*;
use scirs2_core::ndarray::{Array1, Array2};
use scirs2_core::random::SeedableRng;
use sklears_tree::{
    // incremental::{
    //     HoeffdingTree, HoeffdingTreeConfig, IncrementalRandomForest, IncrementalRandomForestConfig,
    //     StreamingTreeModel,
    // },
    DecisionTreeClassifier,
    DecisionTreeRegressor, // ExtraTreesClassifier, ExtraTreesRegressor,
    // GradientBoostingClassifier, GradientBoostingRegressor,
    RandomForestClassifier,
    // RandomForestRegressor, SoftDecisionTreeClassifier, BART,
};

/// Generate test data for classification
fn classification_data_strategy() -> impl Strategy<Value = (Array2<f64>, Array1<f64>)> {
    (
        prop::collection::vec(
            prop::collection::vec(-10.0f64..10.0, 2..10), // features
            10..100,                                      // samples
        ),
        prop::collection::vec(0i32..3, 10..100), // labels (3 classes)
    )
        .prop_filter("Equal lengths", |(features, labels)| {
            features.len() == labels.len()
        })
        .prop_map(|(features, labels)| {
            let n_samples = features.len();
            let n_features = if features.is_empty() {
                2
            } else {
                features[0].len()
            };
            let flat_features: Vec<f64> = features.into_iter().flatten().collect();

            // Ensure the flat_features has the expected length
            let expected_len = n_samples * n_features;
            if flat_features.len() != expected_len {
                // If lengths don't match, generate a properly sized array
                let mut fixed_features = Vec::with_capacity(expected_len);
                for i in 0..n_samples {
                    for j in 0..n_features {
                        let idx = (i * n_features + j) % flat_features.len();
                        fixed_features.push(flat_features.get(idx).copied().unwrap_or(0.0));
                    }
                }
                let X = Array2::from_shape_vec((n_samples, n_features), fixed_features).unwrap();
                let y =
                    Array1::from_vec(labels.into_iter().map(|x| x as f64).collect::<Vec<f64>>());
                (X, y)
            } else {
                let X = Array2::from_shape_vec((n_samples, n_features), flat_features).unwrap();
                let y =
                    Array1::from_vec(labels.into_iter().map(|x| x as f64).collect::<Vec<f64>>());
                (X, y)
            }
        })
}

/// Generate test data for classification with i32 labels (for RandomForest)
fn classification_data_i32_strategy() -> impl Strategy<Value = (Array2<f64>, Array1<i32>)> {
    (
        prop::collection::vec(
            prop::collection::vec(-10.0f64..10.0, 2..10), // features
            10..100,                                      // samples
        ),
        prop::collection::vec(0i32..3, 10..100), // labels (3 classes)
    )
        .prop_filter("Equal lengths", |(features, labels)| {
            features.len() == labels.len()
        })
        .prop_map(|(features, labels)| {
            let n_samples = features.len();
            let n_features = if features.is_empty() {
                2
            } else {
                features[0].len()
            };
            let flat_features: Vec<f64> = features.into_iter().flatten().collect();
            let expected_len = n_samples * n_features;
            if flat_features.len() != expected_len {
                let mut fixed_features = Vec::with_capacity(expected_len);
                for i in 0..n_samples {
                    for j in 0..n_features {
                        let idx = (i * n_features + j) % flat_features.len();
                        fixed_features.push(flat_features.get(idx).copied().unwrap_or(0.0));
                    }
                }
                let X = Array2::from_shape_vec((n_samples, n_features), fixed_features).unwrap();
                let y = Array1::from_vec(labels);
                (X, y)
            } else {
                let X = Array2::from_shape_vec((n_samples, n_features), flat_features).unwrap();
                let y = Array1::from_vec(labels);
                (X, y)
            }
        })
}

/// Generate test data for regression
fn regression_data_strategy() -> impl Strategy<Value = (Array2<f64>, Array1<f64>)> {
    (
        prop::collection::vec(
            prop::collection::vec(-10.0f64..10.0, 2..10), // features
            10..100,                                      // samples
        ),
        prop::collection::vec(-100.0f64..100.0, 10..100), // continuous targets
    )
        .prop_filter("Equal lengths", |(features, targets)| {
            features.len() == targets.len()
        })
        .prop_map(|(features, targets)| {
            let n_samples = features.len();
            let n_features = if features.is_empty() {
                2
            } else {
                features[0].len()
            };
            let flat_features: Vec<f64> = features.into_iter().flatten().collect();

            // Ensure the flat_features has the expected length
            let expected_len = n_samples * n_features;
            if flat_features.len() != expected_len {
                // If lengths don't match, generate a properly sized array
                let mut fixed_features = Vec::with_capacity(expected_len);
                for i in 0..n_samples {
                    for j in 0..n_features {
                        let idx = (i * n_features + j) % flat_features.len();
                        fixed_features.push(flat_features.get(idx).copied().unwrap_or(0.0));
                    }
                }
                let X = Array2::from_shape_vec((n_samples, n_features), fixed_features).unwrap();
                let y = Array1::from_vec(targets);
                (X, y)
            } else {
                let X = Array2::from_shape_vec((n_samples, n_features), flat_features).unwrap();
                let y = Array1::from_vec(targets);
                (X, y)
            }
        })
}

/// Property: Decision tree predictions should be deterministic
proptest! {
    #[test]
    fn decision_tree_classifier_deterministic(
        (X, y) in classification_data_strategy(),
        random_seed in any::<u64>(),
    ) {
        let tree = DecisionTreeClassifier::new()
            .max_depth(5)
            .random_state(Some(random_seed));

        let trained_tree = tree.fit(&X, &y).unwrap();

        // Make predictions twice
        let pred1 = trained_tree.predict(&X).unwrap();
        let pred2 = trained_tree.predict(&X).unwrap();

        // Predictions should be identical
        for (p1, p2) in pred1.iter().zip(pred2.iter()) {
            assert_eq!(p1, p2);
        }
    }
}

/// Property: Decision tree predictions should be within valid range for classification
proptest! {
    #[test]
    fn decision_tree_classifier_valid_predictions(
        (X, y) in classification_data_strategy(),
        random_seed in any::<u64>(),
    ) {
        let tree = DecisionTreeClassifier::new()
            .max_depth(5)
            .random_state(Some(random_seed));

        let trained_tree = tree.fit(&X, &y).unwrap();
        let predictions = trained_tree.predict(&X).unwrap();

        // Get unique classes from training data
        let mut unique_classes: Vec<i32> = y.iter().map(|&x| x as i32).collect();
        unique_classes.sort_unstable();
        unique_classes.dedup();

        // All predictions should be valid class labels
        for pred in predictions.iter() {
            let pred_class = *pred as i32;
            assert!(unique_classes.contains(&pred_class),
                   "Prediction {} not in training classes {:?}", pred_class, unique_classes);
        }
    }
}

/// Property: Decision tree regressor predictions should be reasonable
proptest! {
    #[test]
    fn decision_tree_regressor_reasonable_predictions(
        (X, y) in regression_data_strategy(),
        random_seed in any::<u64>(),
    ) {
        let tree = DecisionTreeRegressor::new()
            .max_depth(5)
            .random_state(Some(random_seed));

        let trained_tree = tree.fit(&X, &y).unwrap();
        let predictions = trained_tree.predict(&X).unwrap();

        // Predictions should be finite
        for pred in predictions.iter() {
            assert!(pred.is_finite(), "Prediction {} is not finite", pred);
        }

        // Predictions should be within a reasonable range of target values
        let y_min = y.iter().cloned().fold(f64::INFINITY, f64::min);
        let y_max = y.iter().cloned().fold(f64::NEG_INFINITY, f64::max);
        let y_range = y_max - y_min;
        let tolerance = y_range * 2.0 + 100.0; // Allow some extrapolation

        for pred in predictions.iter() {
            assert!(*pred >= y_min - tolerance && *pred <= y_max + tolerance,
                   "Prediction {} outside reasonable range [{}, {}]",
                   pred, y_min - tolerance, y_max + tolerance);
        }
    }
}

/// Property: Random forest should have reasonable ensemble properties
proptest! {
    #[test]
    fn random_forest_ensemble_properties(
        (X, y) in classification_data_i32_strategy(),
        n_estimators in 5usize..20,
        random_seed in any::<u64>(),
    ) {
        let rf = RandomForestClassifier::new()
            .n_estimators(n_estimators)
            .max_depth(3)
            .random_state(random_seed);

        let trained_rf = rf.fit(&X, &y).unwrap();
        let predictions = trained_rf.predict(&X).unwrap();

        // All predictions should be valid integers (classification)
        for pred in predictions.iter() {
            let pred_int = *pred as i32;
            assert!(pred_int.abs() < 1000000, "RF prediction {} is unreasonable", pred);
        }

        // Ensemble should reduce variance compared to individual trees
        // (This is a simplified test - in practice you'd compare with single tree)
        let prediction_variance = {
            let mean_pred = predictions.mean().unwrap() as f64;
            predictions.iter()
                .map(|&p| (p as f64 - mean_pred).powi(2))
                .sum::<f64>() / predictions.len() as f64
        };

        // Variance should be reasonable (not infinite or NaN)
        assert!(prediction_variance.is_finite(),
               "RF prediction variance {} is not finite", prediction_variance);
    }
}

/// Property: Feature importance should be non-negative and sum to reasonable value
proptest! {
    #[test]
    fn feature_importance_properties(
        (X, y) in classification_data_i32_strategy(),
        random_seed in any::<u64>(),
    ) {
        let rf = RandomForestClassifier::new()
            .n_estimators(10)
            .max_depth(4)
            .random_state(random_seed);

        let trained_rf = rf.fit(&X, &y).unwrap();
        let importances = trained_rf.feature_importances().unwrap();

        // All importances should be non-negative
        for &importance in importances.iter() {
            assert!(importance >= 0.0,
                   "Feature importance {} is negative", importance);
            assert!(importance.is_finite(),
                   "Feature importance {} is not finite", importance);
        }

        // Sum of importances should be approximately 1.0 (or at least reasonable)
        let importance_sum = importances.sum();
        assert!(importance_sum > 0.0 && importance_sum <= 2.0,
               "Feature importance sum {} is unreasonable", importance_sum);
    }
}

/// Property: Gradient boosting should converge
proptest! {
    #[test]
    #[ignore] // GradientBoostingRegressor not implemented yet
    fn gradient_boosting_convergence(
        (X, y) in regression_data_strategy(),
        n_estimators in 5usize..50,
        learning_rate in 0.01f64..0.5,
        random_seed in any::<u64>(),
    ) {
        let gb = GradientBoostingRegressor::new()
            .n_estimators(n_estimators)
            .learning_rate(learning_rate)
            .max_depth(3)
            .random_state(Some(random_seed));

        let trained_gb = gb.fit(&X, &y).unwrap();
        let predictions = trained_gb.predict(&X).unwrap();

        // Predictions should be finite
        for pred in predictions.iter() {
            assert!(pred.is_finite(), "GB prediction {} is not finite", pred);
        }

        // Calculate training MSE (should be reasonable)
        let mse = predictions.iter()
            .zip(y.iter())
            .map(|(&pred, &target)| (pred - target).powi(2))
            .sum::<f64>() / predictions.len() as f64;

        // MSE should be finite and not extremely large
        assert!(mse.is_finite(), "GB training MSE {} is not finite", mse);

        let target_variance = {
            let mean_target = y.mean().unwrap();
            y.iter().map(|&t| (t - mean_target).powi(2)).sum::<f64>() / y.len() as f64
        };

        // MSE should not be worse than predicting the mean (with some tolerance)
        assert!(mse <= target_variance * 2.0,
               "GB MSE {} is worse than baseline variance {}", mse, target_variance);
    }
}

/// Property: Extra Trees should produce valid predictions
proptest! {
    #[test]
    #[ignore] // ExtraTreesClassifier not implemented yet
    fn extra_trees_validity(
        (X, y) in classification_data_strategy(),
        n_estimators in 5usize..20,
        random_seed in any::<u64>(),
    ) {
        let et = ExtraTreesClassifier::new()
            .n_estimators(n_estimators)
            .max_depth(Some(4))
            .random_state(Some(random_seed));

        let trained_et = et.fit(&X, &y).unwrap();
        let predictions = trained_et.predict(&X).unwrap();

        // All predictions should be finite
        for pred in predictions.iter() {
            let pred_int = *pred as i32;
            assert!(pred_int.abs() < 1000000, "ET prediction {} is unreasonable", pred);
        }

        // Predictions should be valid class labels
        let mut unique_classes: Vec<i32> = y.iter().map(|&x| x as i32).collect();
        unique_classes.sort_unstable();
        unique_classes.dedup();

        for pred in predictions.iter() {
            let pred_class = *pred as i32;
            assert!(unique_classes.contains(&pred_class),
                   "ET prediction {} not in training classes {:?}", pred_class, unique_classes);
        }
    }
}

/// Property: Soft decision trees should produce probabilistic outputs
proptest! {
    #[test]
    #[ignore] // SoftDecisionTreeClassifier not implemented yet
    fn soft_trees_probability_properties(
        (X, y) in classification_data_strategy(),
        temperature in 0.1f64..2.0,
        random_seed in any::<u64>(),
    ) {
        if X.nrows() < 5 || y.len() < 5 {
            return Ok(()); // Skip very small datasets
        }

        let soft_tree = SoftDecisionTreeClassifier::<sklears_core::traits::Untrained>::builder()
            .max_depth(Some(3))
            .temperature(temperature)
            .n_iterations(20) // Keep it small for property testing
            .learning_rate(0.1)
            .random_state(Some(random_seed))
            .build();

        let trained_tree = soft_tree.fit(&X, &y).unwrap();

        // Test probability predictions
        let probabilities = trained_tree.predict_proba(&X).unwrap();

        // Check probability properties
        for i in 0..probabilities.nrows() {
            let row_sum: f64 = probabilities.row(i).sum();

            // Probabilities should sum to approximately 1.0
            assert_abs_diff_eq!(row_sum, 1.0, epsilon = 1e-6);

            // All probabilities should be non-negative
            for &prob in probabilities.row(i).iter() {
                assert!(prob >= 0.0, "Negative probability: {}", prob);
                assert!(prob <= 1.0, "Probability > 1.0: {}", prob);
                assert!(prob.is_finite(), "Non-finite probability: {}", prob);
            }
        }
    }
}

/// Property: BART should provide uncertainty quantification
proptest! {
    #[test]
    #[ignore] // BART not implemented yet
    fn bart_uncertainty_properties(
        (X, y) in regression_data_strategy(),
        n_trees in 5usize..20,
        n_iterations in 10usize..50,
        random_seed in any::<u64>(),
    ) {
        if X.nrows() < 10 || y.len() < 10 {
            return Ok(()); // Skip very small datasets
        }

        let bart = BART::<sklears_core::traits::Untrained>::builder()
            .n_trees(n_trees)
            .n_iterations(n_iterations)
            .n_burn_in(5)
            .max_depth(2) // Keep shallow for property testing
            .random_state(Some(random_seed))
            .build();

        let trained_bart = bart.fit(&X, &y).unwrap();

        // Test uncertainty quantification
        let (means, stds) = trained_bart.predict_with_uncertainty(&X).unwrap();

        // Check uncertainty properties
        for i in 0..means.len() {
            assert!(means[i].is_finite(), "Non-finite BART mean: {}", means[i]);
            assert!(stds[i].is_finite(), "Non-finite BART std: {}", stds[i]);
            assert!(stds[i] >= 0.0, "Negative BART std: {}", stds[i]);
        }

        // Test credible intervals
        let (ci_means, lower, upper) = trained_bart.predict_credible_intervals(&X, 0.9).unwrap();

        for i in 0..ci_means.len() {
            assert!(lower[i] <= upper[i],
                   "Invalid credible interval: [{}, {}]", lower[i], upper[i]);
            assert!(lower[i].is_finite() && upper[i].is_finite(),
                   "Non-finite credible interval bounds");
        }
    }
}

/// Property: Tree depth should be respected
proptest! {
    #[test]
    fn max_depth_respected(
        (X, y) in classification_data_strategy(),
        max_depth in 1usize..6,
        random_seed in any::<u64>(),
    ) {
        let tree = DecisionTreeClassifier::new()
            .max_depth(max_depth)
            .random_state(Some(random_seed));

        let trained_tree = tree.fit(&X, &y).unwrap();

        // Extract tree structure and verify depth
        // This is a simplified check - in practice you'd traverse the actual tree
        let predictions = trained_tree.predict(&X).unwrap();

        // If max_depth is respected, we should get reasonable predictions
        for pred in predictions.iter() {
            let pred_int = *pred as i32;
            assert!(pred_int.abs() < 1000000, "Prediction {} is unreasonable with max_depth={}", pred, max_depth);
        }
    }
}

/// Property: Min samples split should be respected
proptest! {
    #[test]
    fn min_samples_split_respected(
        (X, y) in classification_data_strategy(),
        min_samples_split in 2usize..10,
        random_seed in any::<u64>(),
    ) {
        let tree = DecisionTreeClassifier::new()
            .min_samples_split(min_samples_split)
            .max_depth(5)
            .random_state(Some(random_seed));

        let trained_tree = tree.fit(&X, &y).unwrap();
        let predictions = trained_tree.predict(&X).unwrap();

        // Tree should still produce valid predictions
        for pred in predictions.iter() {
            let pred_int = *pred as i32;
            assert!(pred_int.abs() < 1000000, "Invalid prediction with min_samples_split={}", min_samples_split);
        }
    }
}

/// Property: Training data predictions should be at least as good as random guessing
proptest! {
    #[test]
    fn better_than_random_classification(
        (X, y) in classification_data_i32_strategy(),
        random_seed in any::<u64>(),
    ) {
        if X.nrows() < 10 || y.len() < 10 {
            return Ok(()); // Skip very small datasets
        }

        let rf = RandomForestClassifier::new()
            .n_estimators(10)
            .max_depth(4)
            .random_state(random_seed);

        let trained_rf = rf.fit(&X, &y).unwrap();
        let predictions = trained_rf.predict(&X).unwrap();

        // Calculate accuracy
        let correct_predictions = predictions.iter()
            .zip(y.iter())
            .filter(|(&pred, &target)| (pred as i32) == (target as i32))
            .count();

        let accuracy = correct_predictions as f64 / predictions.len() as f64;

        // Get number of unique classes
        let mut unique_classes: Vec<i32> = y.iter().map(|&x| x as i32).collect();
        unique_classes.sort_unstable();
        unique_classes.dedup();
        let n_classes = unique_classes.len();

        // Accuracy should be at least as good as random guessing (with some tolerance)
        let random_accuracy = 1.0 / n_classes as f64;
        let tolerance = 0.1; // Allow some variance for small datasets

        assert!(accuracy >= random_accuracy - tolerance,
               "Accuracy {} is worse than random guessing {} (n_classes={})",
               accuracy, random_accuracy, n_classes);
    }
}

/// Property: Feature subset selection should work
proptest! {
    #[test]
    fn feature_subset_selection(
        (X, y) in classification_data_i32_strategy(),
        random_seed in any::<u64>(),
    ) {
        let n_features = X.ncols();
        if n_features < 3 {
            return Ok(()); // Skip datasets with too few features
        }

        let max_features = (n_features / 2).max(1);

        use sklears_tree::MaxFeatures;
        let rf = RandomForestClassifier::new()
            .n_estimators(5)
            .max_features(MaxFeatures::Number(max_features))
            .max_depth(3)
            .random_state(random_seed);

        let trained_rf = rf.fit(&X, &y).unwrap();
        let predictions = trained_rf.predict(&X).unwrap();

        // Should still produce valid predictions with feature subset
        for pred in predictions.iter() {
            let pred_int = *pred as i32;
            assert!(pred_int.abs() < 1000000, "Invalid prediction with max_features={}", max_features);
        }

        // Feature importances should respect the constraint
        let importances = trained_rf.feature_importances().unwrap();
        assert_eq!(importances.len(), n_features,
                  "Feature importance length mismatch");
    }
}

/// Property: Incremental learning should maintain consistency
proptest! {
    #[test]
    #[ignore] // HoeffdingTree not implemented yet
    fn incremental_learning_consistency(
        (X, y) in regression_data_strategy(),
        confidence in 0.9f64..0.99,
        random_seed in any::<u64>(),
    ) {
        if X.nrows() < 20 {
            return Ok(()); // Skip very small datasets
        }

        let config = HoeffdingTreeConfig {
            confidence,
            grace_period: 10,
            min_samples_split: 5,
            max_depth: Some(4),
            ..HoeffdingTreeConfig::default()
        };

        let mut tree = HoeffdingTree::new(config, X.ncols());

        // Train incrementally
        for i in 0..X.nrows() {
            let x_sample: Vec<f64> = X.row(i).to_vec();
            let y_sample = y[i];
            tree.update(&x_sample, y_sample, None).unwrap();
        }

        // Test predictions
        let mut all_predictions = Vec::new();
        for i in 0..X.nrows() {
            let x_sample: Vec<f64> = X.row(i).to_vec();
            let pred = tree.predict_single(&x_sample).unwrap();
            all_predictions.push(pred);
        }

        // All predictions should be finite
        for pred in &all_predictions {
            assert!(pred.is_finite(), "Incremental tree prediction {} is not finite", pred);
        }

        // Predictions should be within reasonable range
        let y_min = y.iter().cloned().fold(f64::INFINITY, f64::min);
        let y_max = y.iter().cloned().fold(f64::NEG_INFINITY, f64::max);
        let tolerance = (y_max - y_min) * 2.0 + 100.0;

        for pred in &all_predictions {
            assert!(*pred >= y_min - tolerance && *pred <= y_max + tolerance,
                   "Incremental prediction {} outside reasonable range", pred);
        }
    }
}

/// Property: Tree memory usage should be bounded
proptest! {
    #[test]
    #[ignore] // IncrementalRandomForest not implemented yet
    fn memory_usage_bounded(
        (X, y) in classification_data_strategy(),
        n_estimators in 5usize..15,
        random_seed in any::<u64>(),
    ) {
        if X.nrows() < 10 {
            return Ok(()); // Skip very small datasets
        }

        let config = IncrementalRandomForestConfig {
            n_estimators,
            max_ensemble_size: Some(20),
            ..IncrementalRandomForestConfig::default()
        };

        let mut forest = IncrementalRandomForest::new(config);

        // Convert classification data to regression format for the incremental forest
        let y_reg: Array1<f64> = y.map(|&x| x as f64);
        let weights = Array1::ones(X.nrows());

        // Update the forest
        forest.update(&X, &y_reg, &weights).unwrap();

        // Check that ensemble size is reasonable
        let ensemble_size = forest.get_ensemble_size();
        assert!(ensemble_size <= 20, "Ensemble size {} exceeds maximum", ensemble_size);
        assert!(ensemble_size > 0, "Ensemble size should be positive");

        // Test predictions
        let predictions = forest.predict(&X).unwrap();
        for pred in predictions.iter() {
            assert!(pred.is_finite(), "Forest prediction {} is not finite", pred);
        }
    }
}

/// Property: Predictions should be stable under small data perturbations
proptest! {
    #[test]
    fn prediction_stability_under_perturbation(
        (X, y) in regression_data_strategy(),
        perturbation_scale in 0.001f64..0.1,
        random_seed in any::<u64>(),
    ) {
        if X.nrows() < 10 || X.ncols() < 2 {
            return Ok(()); // Skip very small datasets
        }

        let tree = DecisionTreeRegressor::new()
            .max_depth(4)
            .min_samples_split(3)
            .random_state(Some(random_seed));

        let trained_tree = tree.fit(&X, &y).unwrap();
        let original_predictions = trained_tree.predict(&X).unwrap();

        // Create slightly perturbed data
        let mut X_perturbed = X.clone();
                let mut rng = scirs2_core::random::rngs::StdRng::seed_from_u64(random_seed);

        for i in 0..X_perturbed.nrows() {
            for j in 0..X_perturbed.ncols() {
                let noise = scirs2_core::random::Rng::gen_range(&mut rng, -perturbation_scale..perturbation_scale);
                X_perturbed[[i, j]] += noise;
            }
        }

        let perturbed_predictions = trained_tree.predict(&X_perturbed).unwrap();

        // Check that predictions don't change dramatically
        let max_change = original_predictions.iter()
            .zip(perturbed_predictions.iter())
            .map(|(&orig, &pert)| (orig - pert).abs())
            .fold(0.0f64, f64::max);

        let y_range = y.iter().cloned().fold(f64::NEG_INFINITY, f64::max) -
                     y.iter().cloned().fold(f64::INFINITY, f64::min);
        let reasonable_change = y_range * 2.0; // Allow up to 200% of target range (decision trees can be sensitive)

        assert!(max_change <= reasonable_change,
               "Prediction changed too much: {} > {} under perturbation scale {}",
               max_change, reasonable_change, perturbation_scale);
    }
}

/// Property: Trees should handle outliers gracefully
proptest! {
    #[test]
    fn outlier_robustness(
        (mut X, mut y) in regression_data_strategy(),
        outlier_multiplier in 5.0f64..20.0,
        random_seed in any::<u64>(),
    ) {
        if X.nrows() < 15 {
            return Ok(()); // Skip very small datasets
        }

        // Add some outliers to the data
        let outlier_indices = vec![0, X.nrows() - 1]; // First and last samples

        for &idx in &outlier_indices {
            // Make extreme outliers in features
            for j in 0..X.ncols() {
                X[[idx, j]] *= outlier_multiplier;
            }
            // Make extreme outlier in target
            y[idx] *= outlier_multiplier;
        }

        let tree = DecisionTreeRegressor::new()
            .max_depth(5)
            .min_samples_split(3)
            .random_state(Some(random_seed));

        let trained_tree = tree.fit(&X, &y).unwrap();
        let predictions = trained_tree.predict(&X).unwrap();

        // All predictions should still be finite
        for pred in predictions.iter() {
            assert!(pred.is_finite(), "Tree prediction {} is not finite with outliers", pred);
        }

        // Most predictions should be reasonable (not all extreme outliers)
        let extreme_predictions = predictions.iter()
            .filter(|&&pred| pred.abs() > 1000.0)
            .count();

        let outlier_tolerance = (X.nrows() / 2).max(outlier_indices.len() * 2);
        assert!(extreme_predictions <= outlier_tolerance,
               "Too many extreme predictions {} > {} with outliers",
               extreme_predictions, outlier_tolerance);
    }
}

/// Property: Tree structure should remain valid during incremental updates
proptest! {
    #[test]
    #[ignore] // HoeffdingTree not implemented yet
    fn incremental_tree_structure_validity(
        batch_sizes in prop::collection::vec(5usize..20, 3..8),
        n_features in 2usize..6,
        random_seed in any::<u64>(),
    ) {
        let config = HoeffdingTreeConfig {
            confidence: 0.95,
            grace_period: 10,
            min_samples_split: 3,
            max_depth: Some(3),
            ..HoeffdingTreeConfig::default()
        };

        let mut tree = HoeffdingTree::new(config, n_features);
                let mut rng = scirs2_core::random::rngs::StdRng::seed_from_u64(random_seed);

        let mut total_samples = 0;

        // Add data in batches
        for batch_size in batch_sizes {
            for _ in 0..batch_size {
                let x: Vec<f64> = (0..n_features)
                    .map(|_| scirs2_core::random::Rng::gen_range(&mut rng, -5.0..5.0))
                    .collect();
                let y = scirs2_core::random::Rng::gen_range(&mut rng, 0.0..10.0);

                tree.update(&x, y, None).unwrap();
                total_samples += 1;
            }

            // Test tree statistics after each batch
            let stats = tree.get_stats();

            // Basic invariants
            assert!(stats.n_nodes > 0, "Tree should have at least one node");
            assert!(stats.n_leaves > 0, "Tree should have at least one leaf");
            assert!(stats.n_leaves <= stats.n_nodes, "Leaves should not exceed total nodes");
            assert!(stats.max_depth <= 3, "Tree depth should respect max_depth constraint");

            // Test that we can still make predictions
            let test_x: Vec<f64> = (0..n_features)
                .map(|_| scirs2_core::random::Rng::gen_range(&mut rng, -5.0..5.0))
                .collect();

            let pred = tree.predict_single(&test_x).unwrap();
            assert!(pred.is_finite(), "Tree prediction should be finite after {} samples", total_samples);
        }
    }
}
